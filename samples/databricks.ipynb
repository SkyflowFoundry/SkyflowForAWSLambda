{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Databricks + Skyflow Integration with Unity Catalog\n\nThis notebook demonstrates integrating Skyflow tokenization and detokenization into Databricks using **Unity Catalog Batch Python UDFs**.\n\n## Key Features\n\n- ‚úÖ **Batched execution** - High-throughput batching to Lambda, then Lambda batches to Skyflow\n- ‚úÖ **Persistent functions** - Functions stored in Unity Catalog, available across all clusters\n- ‚úÖ **Governed and shareable** - Fine-grained access control for tokenization and detokenization\n- ‚úÖ **Persistent views** - Create views that automatically tokenize/detokenize data\n- ‚úÖ **Production ready** - Perfect for ETL pipelines, BI tools, and team collaboration\n\n## Architecture\n\n```\nDatabricks ‚Üí Lambda (batched) ‚Üí Skyflow (batched)\n```\n\n- **Credentials**: Managed in Lambda (not in notebooks)\n- **Batching to Lambda**: Configurable (default 500 rows per call for high throughput)\n- **Lambda to Skyflow**: Automatic batching at 25 rows per Skyflow API call\n- **Parallelization**: Spark automatically distributes across partitions\n\n## Prerequisites\n\n1. **Unity Catalog enabled** - Modern Databricks runtime (DBR 13.3+) or SQL warehouse\n2. **Lambda function deployed** - See main README for deployment instructions\n3. **Skyflow credentials** - Cluster ID, Vault ID, Table name\n\n---\n\n# Quick Start\n\n1. **Configure cell 1** with your Lambda URL and Skyflow credentials\n2. **Run cells 2-3** to create the persistent UDFs in Unity Catalog\n3. **Run cells 4+** for usage examples and testing\n\nThe functions persist across cluster restarts and are available to all authorized users!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 1: Configuration - UPDATE THESE VALUES\n# ============================================================================\n\n# Unity Catalog location\nCATALOG = \"your_catalog_name\"\nSCHEMA = \"your_schema_name\"\n\n# Lambda API configuration\nLAMBDA_URL = \"https://YOUR_API_ID.execute-api.YOUR_REGION.amazonaws.com/processDatabricks\"\n\n# Skyflow configuration\nCLUSTER_ID = \"YOUR_CLUSTER_ID\"\nVAULT_ID = \"YOUR_VAULT_ID\"\nTABLE = \"TABLE_NAME\"\n\n# Performance tuning\nBATCH_SIZE = 500  # Rows per Lambda API call (Lambda then batches at 25 rows per Skyflow call)\nREQUEST_TIMEOUT = 30  # HTTP timeout in seconds\n\nprint(\"=\" * 60)\nprint(\"Configuration Summary\")\nprint(\"=\" * 60)\nprint(f\"Catalog:     {CATALOG}\")\nprint(f\"Schema:      {SCHEMA}\")\nprint(f\"Lambda URL:  {LAMBDA_URL}\")\nprint(f\"Cluster ID:  {CLUSTER_ID}\")\nprint(f\"Vault ID:    {VAULT_ID}\")\nprint(f\"Table:       {TABLE}\")\nprint(f\"Batch Size:  {BATCH_SIZE} (to Lambda)\")\nprint(f\"Timeout:     {REQUEST_TIMEOUT}s\")\nprint(\"=\" * 60)\nprint(\"\\n‚úì Configuration loaded\")\nprint(\"\\nNote: Lambda internally batches at 25 rows per Skyflow API call\")\nprint(\"\\nNext: Run cells 2-3 to create batch Python UDFs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 2: Create Tokenization Batch Python UDF\n# ============================================================================\n#\n# This creates a persistent Unity Catalog function using PARAMETER STYLE PANDAS\n# for efficient batched tokenization.\n#\n# IMPORTANT: Due to UC PARAMETER STYLE PANDAS limitations, you cannot pass literal\n# strings directly. Use the derived column pattern shown below.\n#\n\nspark.sql(f\"\"\"\nCREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.skyflow_tokenize_column(\n    column_value STRING,\n    column_name STRING\n)\nRETURNS STRING\nLANGUAGE PYTHON\nPARAMETER STYLE PANDAS\nHANDLER 'tokenize_handler'\nAS $$\nimport pandas as pd\nimport requests\nfrom typing import Iterator, Tuple\n\n# Configuration embedded at function creation time\nLAMBDA_URL = \"{LAMBDA_URL}\"\nCLUSTER_ID = \"{CLUSTER_ID}\"\nVAULT_ID = \"{VAULT_ID}\"\nTABLE = \"{TABLE}\"\nBATCH_SIZE = {BATCH_SIZE}\nREQUEST_TIMEOUT = {REQUEST_TIMEOUT}\n\ndef tokenize_handler(batch_iter: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n    '''\n    Batch tokenization handler using Skyflow Lambda API.\n    \n    Args:\n        batch_iter: Iterator yielding tuples of (values_series, column_names_series)\n    \n    Yields:\n        Series of tokens for each batch\n    '''\n    for values, column_names in batch_iter:\n        if column_names.empty:\n            # No rows in this batch\n            yield column_names\n            continue\n        \n        # Extract column name (constant across batch)\n        col_name = column_names.iloc[0]\n        \n        # Build records list\n        records = []\n        index_map = []  # Track indices of non-null values\n        for idx, v in enumerate(values):\n            if v is not None and pd.notna(v):\n                records.append({{col_name: v}})\n                index_map.append(idx)\n        \n        # If all values are null, return original\n        if not records:\n            yield values\n            continue\n        \n        # Initialize results\n        tokenized = [None] * len(values)\n        \n        # Process in sub-batches\n        start = 0\n        while start < len(records):\n            end = min(start + BATCH_SIZE, len(records))\n            batch_records = records[start:end]\n            batch_indices = index_map[start:end]\n            \n            # Call Lambda API\n            resp = requests.post(\n                LAMBDA_URL,\n                json={{\"records\": batch_records}},\n                headers={{\n                    \"Content-Type\": \"application/json\",\n                    \"X-Skyflow-Operation\": \"tokenize\",\n                    \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                    \"X-Skyflow-Vault-ID\": VAULT_ID,\n                    \"X-Skyflow-Table\": TABLE\n                }},\n                timeout=REQUEST_TIMEOUT\n            )\n            resp.raise_for_status()\n            \n            # Parse response\n            data = resp.json().get(\"data\", [])\n            \n            # Map tokens back to original indices\n            for local_i, rec in enumerate(data):\n                global_idx = batch_indices[local_i]\n                tokenized[global_idx] = rec.get(col_name)\n            \n            start = end\n        \n        # Yield result as Series\n        yield pd.Series(tokenized)\n$$\n\"\"\")\n\nprint(\"‚úì Created batch Python UDF: skyflow_tokenize_column(column_value, column_name)\")\nprint(f\"  Location: {CATALOG}.{SCHEMA}.skyflow_tokenize_column\")\nprint(f\"  Type: PARAMETER STYLE PANDAS (batched execution)\")\nprint(f\"  Batch Size: {BATCH_SIZE} rows per Lambda call\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint()\nprint(\"=\" * 70)\nprint(\"USAGE PATTERN - Derived Column Required\")\nprint(\"=\" * 70)\nprint()\nprint(\"‚ö†Ô∏è  IMPORTANT: You cannot pass literal strings directly to UC Batch Python UDFs.\")\nprint(\"   Use the 'derived column' pattern to work around this limitation:\")\nprint()\nprint(\"‚úó DOESN'T WORK:\")\nprint(\"  SELECT skyflow_tokenize_column(email, 'email') FROM users\")\nprint()\nprint(\"‚úì CORRECT PATTERN:\")\nprint(\"  WITH prepared_data AS (\")\nprint(\"    SELECT email, 'email' AS email_col\")\nprint(\"    FROM users\")\nprint(\"  )\")\nprint(\"  SELECT skyflow_tokenize_column(email, email_col)\")\nprint(\"  FROM prepared_data\")\nprint()\nprint(\"Multiple columns in one query:\")\nprint(\"  WITH prepared_data AS (\")\nprint(\"    SELECT\")\nprint(\"      user_id,\")\nprint(\"      email, 'email' AS email_col,\")\nprint(\"      phone, 'phone' AS phone_col\")\nprint(\"    FROM users\")\nprint(\"  )\")\nprint(\"  SELECT\")\nprint(\"    user_id,\")\nprint(\"    skyflow_tokenize_column(email, email_col) as email_token,\")\nprint(\"    skyflow_tokenize_column(phone, phone_col) as phone_token\")\nprint(\"  FROM prepared_data\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 3: Create Detokenization Batch Python UDF\n# ============================================================================\n#\n# This creates a persistent Unity Catalog function using PARAMETER STYLE PANDAS\n# for efficient batched detokenization.\n#\n\nspark.sql(f\"\"\"\nCREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.skyflow_detokenize(\n    token STRING\n)\nRETURNS STRING\nLANGUAGE PYTHON\nPARAMETER STYLE PANDAS\nHANDLER 'detokenize_handler'\nAS $$\nimport pandas as pd\nimport requests\nfrom typing import Iterator\n\n# Configuration embedded at function creation time\nLAMBDA_URL = \"{LAMBDA_URL}\"\nCLUSTER_ID = \"{CLUSTER_ID}\"\nVAULT_ID = \"{VAULT_ID}\"\nBATCH_SIZE = {BATCH_SIZE}\nREQUEST_TIMEOUT = {REQUEST_TIMEOUT}\n\ndef detokenize_handler(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n    '''\n    Batch detokenization handler using Skyflow Lambda API.\n    \n    Args:\n        batch_iter: Iterator yielding pandas Series (one per batch)\n    \n    Yields:\n        Series of detokenized values for each batch\n    \n    Note: Single-argument UDF receives Iterator[pd.Series], not Iterator[Tuple].\n    '''\n    for tokens in batch_iter:\n        # tokens is a pandas Series for this batch\n        if tokens.empty:\n            yield tokens\n            continue\n        \n        # Build mask for non-null tokens\n        mask = tokens.notna()\n        token_list = tokens[mask].tolist()\n        \n        if not token_list:\n            # All nulls, return original\n            yield tokens\n            continue\n        \n        # Initialize output as copy of input\n        output = tokens.copy()\n        \n        # Process in sub-batches\n        start = 0\n        while start < len(token_list):\n            end = min(start + BATCH_SIZE, len(token_list))\n            batch_tokens = token_list[start:end]\n            \n            # Call Lambda API\n            resp = requests.post(\n                LAMBDA_URL,\n                json={{\"tokens\": batch_tokens}},\n                headers={{\n                    \"Content-Type\": \"application/json\",\n                    \"X-Skyflow-Operation\": \"detokenize\",\n                    \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                    \"X-Skyflow-Vault-ID\": VAULT_ID\n                }},\n                timeout=REQUEST_TIMEOUT\n            )\n            resp.raise_for_status()\n            \n            # Parse response and create token->value mapping\n            data = resp.json().get(\"data\", [])\n            token_to_value = {{r[\"token\"]: r[\"value\"] for r in data}}\n            \n            # Write back into output Series only where not null\n            sub_mask_idx = mask[mask].index[start:end]\n            for idx, tok in zip(sub_mask_idx, batch_tokens):\n                output.at[idx] = token_to_value.get(tok)\n            \n            start = end\n        \n        # Yield result as Series\n        yield output\n$$\n\"\"\")\n\nprint(\"‚úì Created batch Python UDF: skyflow_detokenize(token)\")\nprint(f\"  Location: {CATALOG}.{SCHEMA}.skyflow_detokenize\")\nprint(f\"  Type: PARAMETER STYLE PANDAS (batched execution)\")\nprint(f\"  Batch Size: {BATCH_SIZE} rows per Lambda call\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint(\"\\nThis function is now:\")\nprint(\"  - Persistent in Unity Catalog\")\nprint(\"  - Shareable across workspaces/users\")\nprint(\"  - Usable in SQL queries and persistent views\")\nprint(\"  - Batched for optimal performance\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup Complete!\n\nTwo Unity Catalog Batch Python UDFs have been created:\n\n1. **skyflow_tokenize_column(column_value, column_name)** - Batched tokenization\n2. **skyflow_detokenize(token)** - Batched detokenization\n\nThese functions are now:\n- ‚úÖ Persistent in Unity Catalog\n- ‚úÖ Governed and shareable with proper access control\n- ‚úÖ Callable from SQL queries\n- ‚úÖ Usable in persistent views\n- ‚úÖ Batched for optimal throughput (configurable batch size to Lambda)\n\n**Key Technology:** These functions use `PARAMETER STYLE PANDAS` which enables batched, vectorized execution while maintaining persistence and governance in Unity Catalog.\n\n---\n\n# Usage Examples\n\nThe cells below demonstrate how to use the batch Python UDFs."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "\n",
    "Create sample data for testing the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, current_timestamp\n",
    "\n",
    "# Configure number of test rows\n",
    "NUM_ROWS = 100\n",
    "\n",
    "# Create test data\n",
    "test_df = spark.range(NUM_ROWS).select(\n",
    "    (expr(\"id + 1\").alias(\"user_id\")),\n",
    "    expr(\"concat('user_', id)\").alias(\"username\"),\n",
    "    expr(\"concat('user', id, '@example.com')\").alias(\"email\"),\n",
    "    expr(\"concat('+1-555-', LPAD(id % 1000, 3, '0'), '-', LPAD((id * 7) % 10000, 4, '0'))\").alias(\"phone\"),\n",
    "    current_timestamp().alias(\"created_at\")\n",
    ")\n",
    "\n",
    "# Save as table\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.raw_users\")\n",
    "\n",
    "print(f\"‚úì Created {CATALOG}.{SCHEMA}.raw_users table with {NUM_ROWS} rows\")\n",
    "display(spark.table(f\"{CATALOG}.{SCHEMA}.raw_users\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tokenize Data with Batch Python UDF\n\nUse the batch Python UDF to tokenize email addresses efficiently.\n\n**Note:** Due to Unity Catalog `PARAMETER STYLE PANDAS` limitations, you must use the **derived column pattern** when passing column names. The literal string `'email'` must be converted to a column expression using a subquery.\n\n### Pattern Explanation\n\n```sql\n-- Convert literal 'email' to a column\nSELECT\n    skyflow_tokenize_column(email, column_name) as email_token\nFROM (\n    SELECT email, 'email' AS column_name\n    FROM users\n) t\n```\n\nThe subquery creates a `column_name` column filled with `'email'`, which UC properly converts to a pandas Series in the UDF."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize using the batch Python UDF with derived column pattern\n# Using CTE to ensure column_name is a proper column expression\nresult = spark.sql(f\"\"\"\n    WITH prepared_data AS (\n        SELECT\n            user_id,\n            username,\n            email,\n            phone,\n            created_at,\n            'email' AS column_name\n        FROM {CATALOG}.{SCHEMA}.raw_users\n    )\n    SELECT \n        user_id,\n        username,\n        email,\n        {CATALOG}.{SCHEMA}.skyflow_tokenize_column(email, column_name) as email_token,\n        phone,\n        created_at\n    FROM prepared_data\n\"\"\")\n\n# Save tokenized data\nresult.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.tokenized_users\")\n\nprint(f\"‚úì Created {CATALOG}.{SCHEMA}.tokenized_users table with tokenized emails\")\nprint(f\"  Tokenization was batched at {BATCH_SIZE} rows per Lambda API call\")\ndisplay(spark.table(f\"{CATALOG}.{SCHEMA}.tokenized_users\").limit(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Persistent Detokenized View\n",
    "\n",
    "Create a **persistent view** (not possible with temporary UDFs!) that automatically detokenizes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PERSISTENT view that detokenizes email tokens on-the-fly\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.users_detokenized AS\n",
    "    SELECT \n",
    "        user_id,\n",
    "        username,\n",
    "        {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email,\n",
    "        phone,\n",
    "        created_at\n",
    "    FROM {CATALOG}.{SCHEMA}.tokenized_users\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Created PERSISTENT view: {CATALOG}.{SCHEMA}.users_detokenized\")\n",
    "print(\"\\nüéâ Key Achievement: This is a PERSISTENT view using batched detokenization!\")\n",
    "print(\"   - Not possible with temporary Pandas UDFs\")\n",
    "print(\"   - Much more efficient than scalar UC UDFs\")\n",
    "print(f\"   - Batches at {BATCH_SIZE} rows per API call\")\n",
    "print(\"   - Accessible to all users with permissions\")\n",
    "print(\"\\nQuerying the view:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{SCHEMA}.users_detokenized LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Roundtrip Accuracy\n",
    "\n",
    "Compare original values with tokenized and detokenized values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs detokenized\n",
    "verification_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        t.user_id,\n",
    "        r.email as original_email,\n",
    "        t.email_token,\n",
    "        d.email as detokenized_email,\n",
    "        CASE\n",
    "            WHEN r.email = d.email THEN 'MATCH'\n",
    "            ELSE 'MISMATCH'\n",
    "        END as verification\n",
    "    FROM {CATALOG}.{SCHEMA}.tokenized_users t\n",
    "    JOIN {CATALOG}.{SCHEMA}.raw_users r ON t.user_id = r.user_id\n",
    "    JOIN {CATALOG}.{SCHEMA}.users_detokenized d ON t.user_id = d.user_id\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(verification_df)\n",
    "\n",
    "# Check for any mismatches\n",
    "mismatches = verification_df.filter(\"verification = 'MISMATCH'\").count()\n",
    "if mismatches == 0:\n",
    "    print(\"\\n‚úì All records match! Tokenization ‚Üí Detokenization working correctly.\")\n",
    "    print(\"\\nüéâ Batch Python UDFs are working perfectly with batched Lambda calls!\")\n",
    "else:\n",
    "    print(f\"\\n‚úó Found {mismatches} mismatches - investigate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Create Analytics Dashboard with Detokenization\n\nNow let's create a simple dashboard that demonstrates using the detokenize function in SQL queries for analytics. This shows how your BI tools and dashboards can work seamlessly with Skyflow-protected data.\n\n**Key Benefits:**\n- Query tokenized data tables directly (fast, no PII exposure)\n- Selectively detokenize only when needed (e.g., for display or domain analysis)\n- Use standard SQL with batched detokenization (efficient API usage)\n- Perfect for Databricks SQL dashboards, Tableau, PowerBI, etc.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Dashboard Query 1: Email Domain Distribution\n# ============================================================================\n# This query detokenizes emails to analyze which email domains are most common\n\nprint(\"=\" * 70)\nprint(\"Dashboard: Email Domain Distribution\")\nprint(\"=\" * 70)\nprint(\"Demonstrating: Detokenize ‚Üí Extract domain ‚Üí Aggregate\\n\")\n\ndomain_analysis = spark.sql(f\"\"\"\n    WITH detokenized AS (\n        SELECT\n            user_id,\n            username,\n            {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email\n        FROM {CATALOG}.{SCHEMA}.tokenized_users\n    ),\n    domains AS (\n        SELECT\n            SUBSTRING_INDEX(email, '@', -1) as email_domain,\n            COUNT(*) as user_count\n        FROM detokenized\n        GROUP BY email_domain\n        ORDER BY user_count DESC\n    )\n    SELECT * FROM domains\n\"\"\")\n\ndisplay(domain_analysis)\n\nprint(\"\\n‚úì Email domain analysis complete\")\nprint(\"  This query batched detokenization of all emails efficiently\")\n\n# ============================================================================\n# Dashboard Query 2: Recent User Activity with Selective Detokenization\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Dashboard: Recent User Activity\")\nprint(\"=\" * 70)\nprint(\"Demonstrating: Show tokens by default, detokenize only on demand\\n\")\n\nrecent_users = spark.sql(f\"\"\"\n    SELECT\n        user_id,\n        username,\n        email_token,\n        {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email_plaintext,\n        DATE(created_at) as registration_date\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n    ORDER BY created_at DESC\n    LIMIT 20\n\"\"\")\n\ndisplay(recent_users)\n\nprint(\"\\n‚úì Recent user activity dashboard complete\")\nprint(\"  Shows both tokenized (for auditing) and detokenized (for display) values\")\n\n# ============================================================================\n# Dashboard Query 3: User Summary Statistics\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Dashboard: User Summary Statistics\")\nprint(\"=\" * 70)\nprint(\"Demonstrating: Aggregate analytics without detokenization\\n\")\n\nsummary_stats = spark.sql(f\"\"\"\n    SELECT\n        COUNT(*) as total_users,\n        COUNT(DISTINCT email_token) as unique_emails,\n        DATE(MIN(created_at)) as first_registration,\n        DATE(MAX(created_at)) as last_registration\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n\"\"\")\n\ndisplay(summary_stats)\n\nprint(\"\\n‚úì Summary statistics complete\")\nprint(\"  This query ran entirely on tokenized data - no detokenization needed!\")\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Dashboard Created Successfully!\")\nprint(\"=\" * 70)\nprint(\"\\nüí° Key Insights:\")\nprint(\"   1. Detokenization is batched automatically for efficiency\")\nprint(\"   2. You can mix tokenized and detokenized columns in the same query\")\nprint(\"   3. Many analytics queries don't need detokenization at all\")\nprint(\"   4. Use Unity Catalog permissions to control who can see plaintext data\")\nprint(\"\\nüéØ BI Tool Integration:\")\nprint(\"   - These queries can be saved as Databricks SQL dashboards\")\nprint(\"   - Connect Tableau, PowerBI, or other tools to the detokenized view\")\nprint(\"   - Access control ensures only authorized users see plaintext PII\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create Interactive Dashboard Visualizations\n\nNow let's create visual charts from our queries. These visualizations can be:\n- Viewed interactively in the notebook\n- Scheduled to run automatically (Databricks Jobs)\n- Shared with stakeholders via notebook links\n- Exported to Databricks SQL Dashboards\n\n**Note:** After running the cell below, click the chart icons in the output to configure visualization types (bar charts, pie charts, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Create Dashboard Visualizations\n# ============================================================================\n\nprint(\"=\" * 70)\nprint(\"Creating Interactive Dashboard Visualizations\")\nprint(\"=\" * 70)\nprint()\n\n# Visualization 1: Email Domain Distribution (Bar Chart)\nprint(\"üìä Visualization 1: Email Domain Distribution\")\nprint(\"-\" * 70)\n\ndomain_viz = spark.sql(f\"\"\"\n    WITH detokenized AS (\n        SELECT\n            user_id,\n            {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email\n        FROM {CATALOG}.{SCHEMA}.tokenized_users\n    ),\n    domains AS (\n        SELECT\n            SUBSTRING_INDEX(email, '@', -1) as email_domain,\n            COUNT(*) as user_count\n        FROM detokenized\n        GROUP BY email_domain\n        ORDER BY user_count DESC\n    )\n    SELECT * FROM domains\n\"\"\")\n\ndisplay(domain_viz)\nprint(\"üí° Tip: Click the chart icon above to visualize as a bar chart\")\nprint(\"   X-axis: email_domain, Y-axis: user_count\\n\")\n\n# Visualization 2: User Registration Timeline\nprint(\"üìä Visualization 2: User Registration Timeline\")\nprint(\"-\" * 70)\n\ntimeline_viz = spark.sql(f\"\"\"\n    SELECT\n        DATE(created_at) as registration_date,\n        COUNT(*) as new_users\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n    GROUP BY DATE(created_at)\n    ORDER BY registration_date\n\"\"\")\n\ndisplay(timeline_viz)\nprint(\"üí° Tip: Click the chart icon above to visualize as a line chart\")\nprint(\"   X-axis: registration_date, Y-axis: new_users\\n\")\n\n# Visualization 3: Summary Metrics\nprint(\"üìä Visualization 3: Key Metrics Summary\")\nprint(\"-\" * 70)\n\nmetrics_viz = spark.sql(f\"\"\"\n    SELECT\n        'Total Users' as metric,\n        CAST(COUNT(*) AS STRING) as value\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n    UNION ALL\n    SELECT\n        'Unique Emails' as metric,\n        CAST(COUNT(DISTINCT email_token) AS STRING) as value\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n    UNION ALL\n    SELECT\n        'Days Active' as metric,\n        CAST(DATEDIFF(MAX(created_at), MIN(created_at)) AS STRING) as value\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n\"\"\")\n\ndisplay(metrics_viz)\nprint(\"üí° This shows key performance indicators (KPIs)\\n\")\n\n# Visualization 4: Sample User Records with Detokenization\nprint(\"üìä Visualization 4: Recent Users (with PII)\")\nprint(\"-\" * 70)\nprint(\"‚ö†Ô∏è  This visualization detokenizes PII - ensure proper access controls!\")\n\nrecent_users_viz = spark.sql(f\"\"\"\n    SELECT\n        user_id,\n        username,\n        {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email,\n        DATE(created_at) as registration_date\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n    ORDER BY created_at DESC\n    LIMIT 10\n\"\"\")\n\ndisplay(recent_users_viz)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úì Dashboard Visualizations Created Successfully!\")\nprint(\"=\" * 70)\nprint(\"\\nüìå Next Steps:\")\nprint(\"   1. Configure chart types by clicking the visualization icons\")\nprint(\"   2. Save this notebook and schedule it to run periodically\")\nprint(\"   3. Share the notebook URL with stakeholders\")\nprint(\"   4. Export specific queries to Databricks SQL for persistent dashboards\")\nprint(\"\\nüîí Security Reminder:\")\nprint(\"   - Use Unity Catalog permissions to control who can run this notebook\")\nprint(\"   - Only authorized users should access visualizations with detokenized data\")\nprint(\"   - Consider creating separate dashboards for tokenized vs. detokenized views\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create Custom HTML Dashboard (Optional)\n\nFor a more polished look, you can create a custom HTML dashboard using `displayHTML()`. This is great for executive reports and stakeholder presentations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# Create Custom HTML Dashboard\n# ============================================================================\n\n# Fetch metrics data\nmetrics_data = spark.sql(f\"\"\"\n    SELECT\n        COUNT(*) as total_users,\n        COUNT(DISTINCT email_token) as unique_emails,\n        DATE(MIN(created_at)) as first_registration,\n        DATE(MAX(created_at)) as last_registration\n    FROM {CATALOG}.{SCHEMA}.tokenized_users\n\"\"\").collect()[0]\n\n# Fetch domain distribution\ndomain_data = spark.sql(f\"\"\"\n    WITH detokenized AS (\n        SELECT\n            user_id,\n            {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email\n        FROM {CATALOG}.{SCHEMA}.tokenized_users\n    ),\n    domains AS (\n        SELECT\n            SUBSTRING_INDEX(email, '@', -1) as email_domain,\n            COUNT(*) as user_count\n        FROM detokenized\n        GROUP BY email_domain\n        ORDER BY user_count DESC\n        LIMIT 5\n    )\n    SELECT * FROM domains\n\"\"\").collect()\n\n# Build HTML dashboard\nhtml = f\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <style>\n        body {{\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            padding: 20px;\n            margin: 0;\n        }}\n        .dashboard {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .header {{\n            background: white;\n            border-radius: 10px;\n            padding: 30px;\n            margin-bottom: 20px;\n            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n        }}\n        .header h1 {{\n            margin: 0 0 10px 0;\n            color: #333;\n            font-size: 32px;\n        }}\n        .header p {{\n            margin: 0;\n            color: #666;\n            font-size: 16px;\n        }}\n        .metrics {{\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            margin-bottom: 20px;\n        }}\n        .metric-card {{\n            background: white;\n            border-radius: 10px;\n            padding: 25px;\n            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n            transition: transform 0.2s;\n        }}\n        .metric-card:hover {{\n            transform: translateY(-5px);\n        }}\n        .metric-label {{\n            color: #888;\n            font-size: 14px;\n            text-transform: uppercase;\n            letter-spacing: 1px;\n            margin-bottom: 10px;\n        }}\n        .metric-value {{\n            color: #333;\n            font-size: 36px;\n            font-weight: bold;\n        }}\n        .chart-card {{\n            background: white;\n            border-radius: 10px;\n            padding: 30px;\n            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n            margin-bottom: 20px;\n        }}\n        .chart-card h2 {{\n            margin: 0 0 20px 0;\n            color: #333;\n            font-size: 20px;\n        }}\n        .domain-bar {{\n            display: flex;\n            align-items: center;\n            margin-bottom: 15px;\n        }}\n        .domain-label {{\n            min-width: 150px;\n            color: #666;\n            font-size: 14px;\n        }}\n        .bar-container {{\n            flex: 1;\n            height: 30px;\n            background: #f0f0f0;\n            border-radius: 5px;\n            overflow: hidden;\n            margin: 0 15px;\n        }}\n        .bar-fill {{\n            height: 100%;\n            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n            display: flex;\n            align-items: center;\n            justify-content: flex-end;\n            padding-right: 10px;\n            color: white;\n            font-weight: bold;\n            font-size: 12px;\n            transition: width 0.3s ease;\n        }}\n        .footer {{\n            background: white;\n            border-radius: 10px;\n            padding: 20px;\n            text-align: center;\n            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n            color: #666;\n            font-size: 14px;\n        }}\n        .security-badge {{\n            display: inline-block;\n            background: #10b981;\n            color: white;\n            padding: 5px 15px;\n            border-radius: 20px;\n            font-size: 12px;\n            font-weight: bold;\n            margin-top: 10px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"dashboard\">\n        <div class=\"header\">\n            <h1>Skyflow User Analytics Dashboard</h1>\n            <p>Real-time analytics with secure detokenization powered by Skyflow + Databricks</p>\n        </div>\n        \n        <div class=\"metrics\">\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Total Users</div>\n                <div class=\"metric-value\">{metrics_data.total_users:,}</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Unique Emails</div>\n                <div class=\"metric-value\">{metrics_data.unique_emails:,}</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">First Registration</div>\n                <div class=\"metric-value\" style=\"font-size: 24px;\">{metrics_data.first_registration}</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Latest Registration</div>\n                <div class=\"metric-value\" style=\"font-size: 24px;\">{metrics_data.last_registration}</div>\n            </div>\n        </div>\n        \n        <div class=\"chart-card\">\n            <h2>Top Email Domains</h2>\n            {\"\".join([f'''\n            <div class=\"domain-bar\">\n                <div class=\"domain-label\">{row.email_domain}</div>\n                <div class=\"bar-container\">\n                    <div class=\"bar-fill\" style=\"width: {(row.user_count / domain_data[0].user_count) * 100}%\">\n                        {row.user_count}\n                    </div>\n                </div>\n            </div>\n            ''' for row in domain_data])}\n        </div>\n        \n        <div class=\"footer\">\n            <strong>Powered by Skyflow Data Privacy Vault</strong>\n            <br>\n            All PII is tokenized at rest and detokenized on-demand with batched API calls\n            <div class=\"security-badge\">üîí SECURE & COMPLIANT</div>\n        </div>\n    </div>\n</body>\n</html>\n\"\"\"\n\ndisplayHTML(html)\n\nprint(\"‚úì Custom HTML dashboard created successfully!\")\nprint(\"\\nüí° Benefits of HTML dashboards:\")\nprint(\"   - Professional, polished appearance\")\nprint(\"   - Fully customizable styling and branding\")\nprint(\"   - Can be scheduled and emailed automatically\")\nprint(\"   - Great for executive reports and stakeholder updates\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Publishing to Databricks SQL Dashboards\n\nTo create a persistent Databricks SQL Dashboard from these queries:\n\n### Method 1: Manual Export (Recommended)\n1. Navigate to **Databricks SQL** in your workspace\n2. Create a new **Query** for each visualization:\n   - Copy the SQL query from the cells above\n   - Save as a named query (e.g., \"User Email Domains\")\n3. Go to **Dashboards** ‚Üí **Create Dashboard**\n4. Add your saved queries as widgets\n5. Configure visualizations (bar charts, line charts, etc.)\n6. Set up automatic refresh schedules\n\n### Method 2: Scheduled Notebook\n- Schedule this notebook to run on a regular cadence (hourly, daily, etc.)\n- Share the notebook URL with stakeholders\n- Users can view the latest results by opening the notebook\n\n### Method 3: Databricks Apps (New)\nIf you have access to Databricks Apps, you can create an interactive web application:\n- Export queries as REST API endpoints\n- Build a custom frontend with React/Vue/etc.\n- Deploy as a Databricks App for production use\n\n### Key Advantages of Databricks SQL Dashboards:\n- ‚úÖ **Persistent** - Dashboards survive cluster restarts\n- ‚úÖ **Scheduled refresh** - Automatic data updates\n- ‚úÖ **Access control** - Fine-grained permissions via Unity Catalog\n- ‚úÖ **Sharing** - Easy to share links with stakeholders\n- ‚úÖ **Interactive** - Click to drill down, filter, and explore\n- ‚úÖ **Alerting** - Set up alerts on metric thresholds\n\n### Security Best Practices:\n- Create separate dashboards for tokenized vs. detokenized views\n- Use Unity Catalog permissions to control access to detokenization functions\n- Audit who accesses dashboards with PII using Databricks audit logs\n- Consider using Skyflow's column-level redaction policies for fine-grained control",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Generate Importable Dashboard JSON\n\nCreate a Databricks Lakeview dashboard file (`.lvdash.json`) that can be imported directly into Databricks SQL.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport os\n\n# ============================================================================\n# Generate Databricks Lakeview Dashboard JSON\n# ============================================================================\n\ndashboard_json = {\n    \"datasets\": [\n        {\n            \"name\": \"skyflow_users_detokenized\",\n            \"displayName\": \"Skyflow Users (Detokenized)\",\n            \"queryLines\": [\n                \"SELECT\\n\",\n                \"    user_id,\\n\",\n                \"    username,\\n\",\n                f\"    {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email,\\n\",\n                \"    email_token,\\n\",\n                \"    phone,\\n\",\n                \"    DATE(created_at) as registration_date,\\n\",\n                \"    created_at\\n\",\n                f\"FROM {CATALOG}.{SCHEMA}.tokenized_users;\"\n            ]\n        },\n        {\n            \"name\": \"skyflow_email_domains\",\n            \"displayName\": \"Email Domain Distribution\",\n            \"queryLines\": [\n                \"WITH detokenized AS (\\n\",\n                \"    SELECT\\n\",\n                \"        user_id,\\n\",\n                f\"        {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email\\n\",\n                f\"    FROM {CATALOG}.{SCHEMA}.tokenized_users\\n\",\n                \"),\\n\",\n                \"domains AS (\\n\",\n                \"    SELECT\\n\",\n                \"        SUBSTRING_INDEX(email, '@', -1) as email_domain,\\n\",\n                \"        COUNT(*) as user_count\\n\",\n                \"    FROM detokenized\\n\",\n                \"    GROUP BY email_domain\\n\",\n                \")\\n\",\n                \"SELECT * FROM domains\\n\",\n                \"ORDER BY user_count DESC;\"\n            ]\n        },\n        {\n            \"name\": \"skyflow_summary_metrics\",\n            \"displayName\": \"Summary Metrics\",\n            \"queryLines\": [\n                \"SELECT\\n\",\n                \"    COUNT(*) as total_users,\\n\",\n                \"    COUNT(DISTINCT email_token) as unique_emails,\\n\",\n                \"    DATE(MIN(created_at)) as first_registration,\\n\",\n                \"    DATE(MAX(created_at)) as last_registration,\\n\",\n                \"    DATEDIFF(MAX(created_at), MIN(created_at)) as days_active\\n\",\n                f\"FROM {CATALOG}.{SCHEMA}.tokenized_users;\"\n            ]\n        },\n        {\n            \"name\": \"skyflow_registration_timeline\",\n            \"displayName\": \"Registration Timeline\",\n            \"queryLines\": [\n                \"SELECT\\n\",\n                \"    DATE(created_at) as registration_date,\\n\",\n                \"    COUNT(*) as new_users\\n\",\n                f\"FROM {CATALOG}.{SCHEMA}.tokenized_users\\n\",\n                \"GROUP BY DATE(created_at)\\n\",\n                \"ORDER BY registration_date;\"\n            ]\n        }\n    ],\n    \"pages\": [\n        {\n            \"name\": \"skyflow_analytics\",\n            \"displayName\": \"Skyflow User Analytics\",\n            \"layout\": [\n                {\n                    \"widget\": {\n                        \"name\": \"header\",\n                        \"multilineTextboxSpec\": {\n                            \"lines\": [\n                                \"\\n\",\n                                \"# Skyflow User Analytics Dashboard\\n\",\n                                \"Secure analytics with on-demand detokenization powered by Skyflow + Databricks Unity Catalog\\n\",\n                                \"üîí All PII is tokenized at rest and detokenized in batches for optimal performance\\n\"\n                            ]\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 0,\n                        \"y\": 0,\n                        \"width\": 6,\n                        \"height\": 2\n                    }\n                },\n                {\n                    \"widget\": {\n                        \"name\": \"total_users_metric\",\n                        \"queries\": [\n                            {\n                                \"name\": \"main_query\",\n                                \"query\": {\n                                    \"datasetName\": \"skyflow_summary_metrics\",\n                                    \"fields\": [\n                                        {\n                                            \"name\": \"total_users\",\n                                            \"expression\": \"`total_users`\"\n                                        }\n                                    ],\n                                    \"disaggregated\": True\n                                }\n                            }\n                        ],\n                        \"spec\": {\n                            \"version\": 2,\n                            \"widgetType\": \"counter\",\n                            \"encodings\": {\n                                \"value\": {\n                                    \"fieldName\": \"total_users\",\n                                    \"displayName\": \"Total Users\"\n                                }\n                            },\n                            \"frame\": {\n                                \"title\": \"Total Users\",\n                                \"showTitle\": True\n                            }\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 0,\n                        \"y\": 2,\n                        \"width\": 2,\n                        \"height\": 2\n                    }\n                },\n                {\n                    \"widget\": {\n                        \"name\": \"unique_emails_metric\",\n                        \"queries\": [\n                            {\n                                \"name\": \"main_query\",\n                                \"query\": {\n                                    \"datasetName\": \"skyflow_summary_metrics\",\n                                    \"fields\": [\n                                        {\n                                            \"name\": \"unique_emails\",\n                                            \"expression\": \"`unique_emails`\"\n                                        }\n                                    ],\n                                    \"disaggregated\": True\n                                }\n                            }\n                        ],\n                        \"spec\": {\n                            \"version\": 2,\n                            \"widgetType\": \"counter\",\n                            \"encodings\": {\n                                \"value\": {\n                                    \"fieldName\": \"unique_emails\",\n                                    \"displayName\": \"Unique Emails\"\n                                }\n                            },\n                            \"frame\": {\n                                \"title\": \"Unique Email Addresses\",\n                                \"showTitle\": True\n                            }\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 2,\n                        \"y\": 2,\n                        \"width\": 2,\n                        \"height\": 2\n                    }\n                },\n                {\n                    \"widget\": {\n                        \"name\": \"days_active_metric\",\n                        \"queries\": [\n                            {\n                                \"name\": \"main_query\",\n                                \"query\": {\n                                    \"datasetName\": \"skyflow_summary_metrics\",\n                                    \"fields\": [\n                                        {\n                                            \"name\": \"days_active\",\n                                            \"expression\": \"`days_active`\"\n                                        }\n                                    ],\n                                    \"disaggregated\": True\n                                }\n                            }\n                        ],\n                        \"spec\": {\n                            \"version\": 2,\n                            \"widgetType\": \"counter\",\n                            \"encodings\": {\n                                \"value\": {\n                                    \"fieldName\": \"days_active\",\n                                    \"displayName\": \"Days Active\"\n                                }\n                            },\n                            \"frame\": {\n                                \"title\": \"Days of Activity\",\n                                \"showTitle\": True\n                            }\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 4,\n                        \"y\": 2,\n                        \"width\": 2,\n                        \"height\": 2\n                    }\n                },\n                {\n                    \"widget\": {\n                        \"name\": \"email_domains_bar\",\n                        \"queries\": [\n                            {\n                                \"name\": \"main_query\",\n                                \"query\": {\n                                    \"datasetName\": \"skyflow_email_domains\",\n                                    \"fields\": [\n                                        {\n                                            \"name\": \"email_domain\",\n                                            \"expression\": \"`email_domain`\"\n                                        },\n                                        {\n                                            \"name\": \"user_count\",\n                                            \"expression\": \"`user_count`\"\n                                        }\n                                    ],\n                                    \"disaggregated\": True\n                                }\n                            }\n                        ],\n                        \"spec\": {\n                            \"version\": 3,\n                            \"widgetType\": \"bar\",\n                            \"encodings\": {\n                                \"x\": {\n                                    \"fieldName\": \"email_domain\",\n                                    \"scale\": {\n                                        \"type\": \"categorical\",\n                                        \"sort\": {\n                                            \"by\": \"y-reversed\"\n                                        }\n                                    },\n                                    \"displayName\": \"Email Domain\"\n                                },\n                                \"y\": {\n                                    \"fieldName\": \"user_count\",\n                                    \"scale\": {\n                                        \"type\": \"quantitative\"\n                                    },\n                                    \"displayName\": \"Number of Users\"\n                                }\n                            },\n                            \"frame\": {\n                                \"title\": \"User Distribution by Email Domain\",\n                                \"showTitle\": True,\n                                \"description\": \"Shows which email domains are most common among registered users\"\n                            }\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 0,\n                        \"y\": 4,\n                        \"width\": 3,\n                        \"height\": 4\n                    }\n                },\n                {\n                    \"widget\": {\n                        \"name\": \"registration_timeline\",\n                        \"queries\": [\n                            {\n                                \"name\": \"main_query\",\n                                \"query\": {\n                                    \"datasetName\": \"skyflow_registration_timeline\",\n                                    \"fields\": [\n                                        {\n                                            \"name\": \"registration_date\",\n                                            \"expression\": \"`registration_date`\"\n                                        },\n                                        {\n                                            \"name\": \"new_users\",\n                                            \"expression\": \"`new_users`\"\n                                        }\n                                    ],\n                                    \"disaggregated\": True\n                                }\n                            }\n                        ],\n                        \"spec\": {\n                            \"version\": 3,\n                            \"widgetType\": \"line\",\n                            \"encodings\": {\n                                \"x\": {\n                                    \"fieldName\": \"registration_date\",\n                                    \"scale\": {\n                                        \"type\": \"temporal\"\n                                    },\n                                    \"displayName\": \"Registration Date\"\n                                },\n                                \"y\": {\n                                    \"fieldName\": \"new_users\",\n                                    \"scale\": {\n                                        \"type\": \"quantitative\"\n                                    },\n                                    \"displayName\": \"New Users\"\n                                }\n                            },\n                            \"frame\": {\n                                \"title\": \"User Registration Timeline\",\n                                \"showTitle\": True,\n                                \"description\": \"Daily new user registrations over time\"\n                            }\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 3,\n                        \"y\": 4,\n                        \"width\": 3,\n                        \"height\": 4\n                    }\n                },\n                {\n                    \"widget\": {\n                        \"name\": \"recent_users_table\",\n                        \"queries\": [\n                            {\n                                \"name\": \"main_query\",\n                                \"query\": {\n                                    \"datasetName\": \"skyflow_users_detokenized\",\n                                    \"fields\": [\n                                        {\n                                            \"name\": \"user_id\",\n                                            \"expression\": \"`user_id`\"\n                                        },\n                                        {\n                                            \"name\": \"username\",\n                                            \"expression\": \"`username`\"\n                                        },\n                                        {\n                                            \"name\": \"email\",\n                                            \"expression\": \"`email`\"\n                                        },\n                                        {\n                                            \"name\": \"email_token\",\n                                            \"expression\": \"`email_token`\"\n                                        },\n                                        {\n                                            \"name\": \"registration_date\",\n                                            \"expression\": \"`registration_date`\"\n                                        }\n                                    ],\n                                    \"disaggregated\": True\n                                }\n                            }\n                        ],\n                        \"spec\": {\n                            \"version\": 2,\n                            \"widgetType\": \"table\",\n                            \"encodings\": {\n                                \"columns\": [\n                                    {\n                                        \"fieldName\": \"user_id\",\n                                        \"displayName\": \"User ID\"\n                                    },\n                                    {\n                                        \"fieldName\": \"username\",\n                                        \"displayName\": \"Username\"\n                                    },\n                                    {\n                                        \"fieldName\": \"email\",\n                                        \"displayName\": \"Email (Detokenized)\"\n                                    },\n                                    {\n                                        \"fieldName\": \"email_token\",\n                                        \"displayName\": \"Email Token\"\n                                    },\n                                    {\n                                        \"fieldName\": \"registration_date\",\n                                        \"displayName\": \"Registration Date\"\n                                    }\n                                ]\n                            },\n                            \"frame\": {\n                                \"title\": \"Recent Users (with Detokenized PII)\",\n                                \"showTitle\": True,\n                                \"description\": \"‚ö†Ô∏è Contains detokenized PII - Access controlled via Unity Catalog permissions\"\n                            }\n                        }\n                    },\n                    \"position\": {\n                        \"x\": 0,\n                        \"y\": 8,\n                        \"width\": 6,\n                        \"height\": 5\n                    }\n                }\n            ],\n            \"pageType\": \"PAGE_TYPE_CANVAS\"\n        }\n    ]\n}\n\n# Convert to formatted JSON string\ndashboard_json_str = json.dumps(dashboard_json, indent=2)\n\n# Save to local /tmp first\ntmp_path = \"/tmp/skyflow_analytics_dashboard.lvdash.json\"\nwith open(tmp_path, 'w') as f:\n    f.write(dashboard_json_str)\n\nprint(\"=\" * 70)\nprint(\"‚úì Databricks Lakeview Dashboard JSON Generated!\")\nprint(\"=\" * 70)\nprint(f\"\\nüìä Dashboard Configuration:\")\nprint(f\"   - Name: Skyflow User Analytics\")\nprint(f\"   - Datasets: {len(dashboard_json['datasets'])}\")\nprint(f\"   - Widgets: {len(dashboard_json['pages'][0]['layout'])}\")\nprint(f\"   - Catalog: {CATALOG}\")\nprint(f\"   - Schema: {SCHEMA}\")\n\nprint(f\"\\nüì• How to Use This Dashboard:\")\nprint(f\"\\n   Option 1: Copy JSON from output below (manual)\")\nprint(f\"   Option 2: Automatically save to DBFS (uncomment Option 2 code)\")\nprint(f\"   Option 3: Automatically import to Workspace (uncomment Option 3 code)\")\n\nprint(f\"\\nüìÑ Complete Dashboard JSON:\")\nprint(\"=\" * 70)\nprint(dashboard_json_str)\nprint(\"=\" * 70)\n\n# ============================================================================\n# OPTION 2: Save to DBFS (Recommended for artifacts)\n# ============================================================================\n# Uncomment the lines below to automatically save to DBFS and get a download link\n\n# dbfs_path = \"dbfs:/FileStore/skyflow/skyflow_analytics_dashboard.lvdash.json\"\n# dbutils.fs.cp(f\"file:{tmp_path}\", dbfs_path, True)\n# print(f\"\\n‚úì OPTION 2 COMPLETE: Saved to DBFS\")\n# print(f\"   DBFS Path: {dbfs_path}\")\n# print(f\"   Download URL: /files/skyflow/skyflow_analytics_dashboard.lvdash.json\")\n# print(f\"   Access from browser: <your-databricks-url>/files/skyflow/skyflow_analytics_dashboard.lvdash.json\")\n\n# ============================================================================\n# OPTION 3: Import to Workspace (makes it visible alongside notebooks)\n# ============================================================================\n# Uncomment the lines below to automatically import into your Workspace\n# Note: Update the email/path to match your Databricks username\n\n# try:\n#     # Get current user (works in most Databricks environments)\n#     current_user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n#     workspace_path = f\"/Users/{current_user}/skyflow_analytics_dashboard.lvdash.json\"\n# except:\n#     # Fallback: manually specify your email\n#     workspace_path = \"/Users/your.email@company.com/skyflow_analytics_dashboard.lvdash.json\"\n# \n# with open(tmp_path, \"r\", encoding=\"utf-8\") as f:\n#     content = f.read()\n# \n# # Import into Workspace (will overwrite if exists)\n# dbutils.workspace.import_(workspace_path, content, overwrite=True)\n# print(f\"\\n‚úì OPTION 3 COMPLETE: Imported to Workspace\")\n# print(f\"   Workspace Path: {workspace_path}\")\n# print(f\"   Navigate to Workspace ‚Üí Users ‚Üí {current_user} to find the file\")\n\nprint(f\"\\nüíæ Manual Import Instructions:\")\nprint(f\"   1. Copy the JSON above (between the === lines)\")\nprint(f\"   2. Save to a local file: skyflow_analytics_dashboard.lvdash.json\")\nprint(f\"   3. Navigate to Databricks SQL ‚Üí Dashboards ‚Üí Import\")\nprint(f\"   4. Upload the .lvdash.json file\")\n\nprint(f\"\\nüé® Dashboard Includes:\")\nprint(f\"   ‚úì Header with security description\")\nprint(f\"   ‚úì 3 metric counters (Total Users, Unique Emails, Days Active)\")\nprint(f\"   ‚úì Email domain distribution bar chart\")\nprint(f\"   ‚úì User registration timeline (line chart)\")\nprint(f\"   ‚úì Recent users table with detokenized PII\")\n\nprint(f\"\\nüîí Security Note:\")\nprint(f\"   Only users with EXECUTE permissions on {CATALOG}.{SCHEMA}.skyflow_detokenize()\")\nprint(f\"   can view detokenized data in this dashboard\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úì Dashboard JSON ready! Choose your preferred option above.\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grant Permissions (Optional)\n",
    "\n",
    "Control access to the batch Python UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Grant function execution permissions\n# Uncomment and adjust as needed\n\n# spark.sql(f\"GRANT EXECUTE ON FUNCTION {CATALOG}.{SCHEMA}.skyflow_tokenize_column TO `data_engineers`\")\n# spark.sql(f\"GRANT EXECUTE ON FUNCTION {CATALOG}.{SCHEMA}.skyflow_detokenize TO `data_engineers`\")\n# spark.sql(f\"GRANT SELECT ON VIEW {CATALOG}.{SCHEMA}.users_detokenized TO `analysts`\")\n\nprint(\"üí° Tip: Use Unity Catalog's access control to govern who can:\")\nprint(\"   - Execute tokenization (potentially create PII tokens)\")\nprint(\"   - Execute detokenization (access plaintext PII)\")\nprint(\"   - Query detokenized views (read PII)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "To remove all test resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment to drop test tables, views, and functions\n# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.raw_users\")\n# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.tokenized_users\")\n# spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{SCHEMA}.users_detokenized\")\n# spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{SCHEMA}.skyflow_detokenize\")\n# spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{SCHEMA}.skyflow_tokenize_column\")\n# print(\"‚úì Cleanup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated **Unity Catalog Batch Python UDFs** for Skyflow integration in Databricks.\n\n### Key Achievements\n\n‚úÖ **Batched Execution**\n- Configurable batch size to Lambda (default 500 rows per call for high throughput)\n- Lambda internally batches at 25 rows per Skyflow API call\n- Dramatically reduces API costs compared to row-by-row processing\n\n‚úÖ **Persistent & Governed**\n- Functions stored in Unity Catalog\n- Shareable across workspaces and users\n- Fine-grained access control\n\n‚úÖ **Production Ready**\n- Usable in persistent views\n- Callable from SQL queries\n- Survives cluster restarts\n- Perfect for BI tools and team collaboration\n\n### The Magic: PARAMETER STYLE PANDAS\n\n```sql\nCREATE FUNCTION ... \nLANGUAGE PYTHON\nPARAMETER STYLE PANDAS  -- This enables batched execution!\nHANDLER 'handler_function'\n```\n\nThis directive tells Databricks to:\n1. Group rows into batches (pandas Series)\n2. Call the handler function with batches (not individual rows)\n3. Process results as batches\n\n### Handler Signature Patterns\n\n**Single-argument UDF** (e.g., detokenization):\n```python\ndef handler(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n    for values in batch_iter:\n        # Process batch\n        yield pd.Series(results)\n```\n\n**Two-argument UDF** (e.g., tokenization):\n```python\ndef handler(batch_iter: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n    for arg1, arg2 in batch_iter:\n        # Process batch\n        yield pd.Series(results)\n```\n\n### Performance Notes\n\n- **Batch Size to Lambda:** Default 500 rows per call (configurable via BATCH_SIZE parameter)\n- **Lambda to Skyflow:** Automatic internal batching at 25 rows per Skyflow API call\n- **Timeout:** 30 seconds (must be ‚â§ Lambda timeout)\n- **Parallelization:** Spark automatically parallelizes across partitions\n- **Scalability:** High batch size to Lambda allows Lambda to scale out and churn through data quickly\n\nFor 100K rows with BATCH_SIZE=500:\n- ~200 Lambda calls\n- Lambda then makes ~4,000 Skyflow API calls (at 25 rows each)\n- Compare to row-by-row scalar UDFs: 100,000 Lambda calls (500x more expensive!)\n\n### Derived Column Pattern\n\nDue to Unity Catalog limitations, you cannot pass literal strings directly to `PARAMETER STYLE PANDAS` functions. Use the derived column pattern:\n\n```sql\n-- Convert literal to column\nWITH prepared AS (\n  SELECT email, 'email' AS email_col\n  FROM users\n)\nSELECT skyflow_tokenize_column(email, email_col)\nFROM prepared\n```\n\n### Next Steps\n\n1. **Monitor Performance:** Check Lambda CloudWatch logs for batch sizes and timing\n2. **Tune Batch Size:** Adjust BATCH_SIZE based on your data size, network latency, and Lambda timeout\n3. **Set Up Governance:** Grant appropriate permissions to users/groups\n4. **Create Views:** Build persistent views for your BI tools and analysts"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}