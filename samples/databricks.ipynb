{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Databricks + Skyflow Integration with Unity Catalog\n\nThis notebook demonstrates integrating Skyflow tokenization and detokenization into Databricks using **Unity Catalog Batch Python UDFs**.\n\n## Key Features\n\n- âœ… **Batched execution** - High-throughput batching to Lambda, then Lambda batches to Skyflow\n- âœ… **Persistent functions** - Functions stored in Unity Catalog, available across all clusters\n- âœ… **Governed and shareable** - Fine-grained access control for tokenization and detokenization\n- âœ… **Persistent views** - Create views that automatically tokenize/detokenize data\n- âœ… **Production ready** - Perfect for ETL pipelines, BI tools, and team collaboration\n\n## Architecture\n\n```\nDatabricks â†’ Lambda (batched) â†’ Skyflow (batched)\n```\n\n- **Credentials**: Managed in Lambda (not in notebooks)\n- **Batching to Lambda**: Configurable (default 500 rows per call for high throughput)\n- **Lambda to Skyflow**: Automatic batching at 25 rows per Skyflow API call\n- **Parallelization**: Spark automatically distributes across partitions\n\n## Prerequisites\n\n1. **Unity Catalog enabled** - Modern Databricks runtime (DBR 13.3+) or SQL warehouse\n2. **Lambda function deployed** - See main README for deployment instructions\n3. **Skyflow credentials** - Cluster ID, Vault ID, Table name\n\n---\n\n# Quick Start\n\n1. **Configure cell 1** with your Lambda URL and Skyflow credentials\n2. **Run cells 2-3** to create the persistent UDFs in Unity Catalog\n3. **Run cells 4+** for usage examples and testing\n\nThe functions persist across cluster restarts and are available to all authorized users!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 1: Configuration - UPDATE THESE VALUES\n# ============================================================================\n\n# Unity Catalog location\nCATALOG = \"your_catalog_name\"\nSCHEMA = \"your_schema_name\"\n\n# Lambda API configuration\nLAMBDA_URL = \"https://YOUR_API_ID.execute-api.YOUR_REGION.amazonaws.com/processDatabricks\"\n\n# Skyflow configuration\nCLUSTER_ID = \"YOUR_CLUSTER_ID\"\nVAULT_ID = \"YOUR_VAULT_ID\"\nTABLE = \"TABLE_NAME\"\n\n# Performance tuning\nBATCH_SIZE = 500  # Rows per Lambda API call (Lambda then batches at 25 rows per Skyflow call)\nREQUEST_TIMEOUT = 30  # HTTP timeout in seconds\n\nprint(\"=\" * 60)\nprint(\"Configuration Summary\")\nprint(\"=\" * 60)\nprint(f\"Catalog:     {CATALOG}\")\nprint(f\"Schema:      {SCHEMA}\")\nprint(f\"Lambda URL:  {LAMBDA_URL}\")\nprint(f\"Cluster ID:  {CLUSTER_ID}\")\nprint(f\"Vault ID:    {VAULT_ID}\")\nprint(f\"Table:       {TABLE}\")\nprint(f\"Batch Size:  {BATCH_SIZE} (to Lambda)\")\nprint(f\"Timeout:     {REQUEST_TIMEOUT}s\")\nprint(\"=\" * 60)\nprint(\"\\nâœ“ Configuration loaded\")\nprint(\"\\nNote: Lambda internally batches at 25 rows per Skyflow API call\")\nprint(\"\\nNext: Run cells 2-3 to create batch Python UDFs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 2: Create Tokenization Batch Python UDF\n# ============================================================================\n#\n# This creates a persistent Unity Catalog function using PARAMETER STYLE PANDAS\n# for efficient batched tokenization.\n#\n# IMPORTANT: Due to UC PARAMETER STYLE PANDAS limitations, you cannot pass literal\n# strings directly. Use the derived column pattern shown below.\n#\n\nspark.sql(f\"\"\"\nCREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.skyflow_tokenize_column(\n    column_value STRING,\n    column_name STRING\n)\nRETURNS STRING\nLANGUAGE PYTHON\nPARAMETER STYLE PANDAS\nHANDLER 'tokenize_handler'\nAS $$\nimport pandas as pd\nimport requests\nfrom typing import Iterator, Tuple\n\n# Configuration embedded at function creation time\nLAMBDA_URL = \"{LAMBDA_URL}\"\nCLUSTER_ID = \"{CLUSTER_ID}\"\nVAULT_ID = \"{VAULT_ID}\"\nTABLE = \"{TABLE}\"\nBATCH_SIZE = {BATCH_SIZE}\nREQUEST_TIMEOUT = {REQUEST_TIMEOUT}\n\ndef tokenize_handler(batch_iter: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n    '''\n    Batch tokenization handler using Skyflow Lambda API.\n    \n    Args:\n        batch_iter: Iterator yielding tuples of (values_series, column_names_series)\n    \n    Yields:\n        Series of tokens for each batch\n    '''\n    for values, column_names in batch_iter:\n        if column_names.empty:\n            # No rows in this batch\n            yield column_names\n            continue\n        \n        # Extract column name (constant across batch)\n        col_name = column_names.iloc[0]\n        \n        # Build records list\n        records = []\n        index_map = []  # Track indices of non-null values\n        for idx, v in enumerate(values):\n            if v is not None and pd.notna(v):\n                records.append({{col_name: v}})\n                index_map.append(idx)\n        \n        # If all values are null, return original\n        if not records:\n            yield values\n            continue\n        \n        # Initialize results\n        tokenized = [None] * len(values)\n        \n        # Process in sub-batches\n        start = 0\n        while start < len(records):\n            end = min(start + BATCH_SIZE, len(records))\n            batch_records = records[start:end]\n            batch_indices = index_map[start:end]\n            \n            # Call Lambda API\n            resp = requests.post(\n                LAMBDA_URL,\n                json={{\"records\": batch_records}},\n                headers={{\n                    \"Content-Type\": \"application/json\",\n                    \"X-Skyflow-Operation\": \"tokenize\",\n                    \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                    \"X-Skyflow-Vault-ID\": VAULT_ID,\n                    \"X-Skyflow-Table\": TABLE\n                }},\n                timeout=REQUEST_TIMEOUT\n            )\n            resp.raise_for_status()\n            \n            # Parse response\n            data = resp.json().get(\"data\", [])\n            \n            # Map tokens back to original indices\n            for local_i, rec in enumerate(data):\n                global_idx = batch_indices[local_i]\n                tokenized[global_idx] = rec.get(col_name)\n            \n            start = end\n        \n        # Yield result as Series\n        yield pd.Series(tokenized)\n$$\n\"\"\")\n\nprint(\"âœ“ Created batch Python UDF: skyflow_tokenize_column(column_value, column_name)\")\nprint(f\"  Location: {CATALOG}.{SCHEMA}.skyflow_tokenize_column\")\nprint(f\"  Type: PARAMETER STYLE PANDAS (batched execution)\")\nprint(f\"  Batch Size: {BATCH_SIZE} rows per Lambda call\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint()\nprint(\"=\" * 70)\nprint(\"USAGE PATTERN - Derived Column Required\")\nprint(\"=\" * 70)\nprint()\nprint(\"âš ï¸  IMPORTANT: You cannot pass literal strings directly to UC Batch Python UDFs.\")\nprint(\"   Use the 'derived column' pattern to work around this limitation:\")\nprint()\nprint(\"âœ— DOESN'T WORK:\")\nprint(\"  SELECT skyflow_tokenize_column(email, 'email') FROM users\")\nprint()\nprint(\"âœ“ CORRECT PATTERN:\")\nprint(\"  WITH prepared_data AS (\")\nprint(\"    SELECT email, 'email' AS email_col\")\nprint(\"    FROM users\")\nprint(\"  )\")\nprint(\"  SELECT skyflow_tokenize_column(email, email_col)\")\nprint(\"  FROM prepared_data\")\nprint()\nprint(\"Multiple columns in one query:\")\nprint(\"  WITH prepared_data AS (\")\nprint(\"    SELECT\")\nprint(\"      user_id,\")\nprint(\"      email, 'email' AS email_col,\")\nprint(\"      phone, 'phone' AS phone_col\")\nprint(\"    FROM users\")\nprint(\"  )\")\nprint(\"  SELECT\")\nprint(\"    user_id,\")\nprint(\"    skyflow_tokenize_column(email, email_col) as email_token,\")\nprint(\"    skyflow_tokenize_column(phone, phone_col) as phone_token\")\nprint(\"  FROM prepared_data\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 3: Create Detokenization Batch Python UDF\n# ============================================================================\n#\n# This creates a persistent Unity Catalog function using PARAMETER STYLE PANDAS\n# for efficient batched detokenization.\n#\n\nspark.sql(f\"\"\"\nCREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.skyflow_detokenize(\n    token STRING\n)\nRETURNS STRING\nLANGUAGE PYTHON\nPARAMETER STYLE PANDAS\nHANDLER 'detokenize_handler'\nAS $$\nimport pandas as pd\nimport requests\nfrom typing import Iterator\n\n# Configuration embedded at function creation time\nLAMBDA_URL = \"{LAMBDA_URL}\"\nCLUSTER_ID = \"{CLUSTER_ID}\"\nVAULT_ID = \"{VAULT_ID}\"\nBATCH_SIZE = {BATCH_SIZE}\nREQUEST_TIMEOUT = {REQUEST_TIMEOUT}\n\ndef detokenize_handler(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n    '''\n    Batch detokenization handler using Skyflow Lambda API.\n    \n    Args:\n        batch_iter: Iterator yielding pandas Series (one per batch)\n    \n    Yields:\n        Series of detokenized values for each batch\n    \n    Note: Single-argument UDF receives Iterator[pd.Series], not Iterator[Tuple].\n    '''\n    for tokens in batch_iter:\n        # tokens is a pandas Series for this batch\n        if tokens.empty:\n            yield tokens\n            continue\n        \n        # Build mask for non-null tokens\n        mask = tokens.notna()\n        token_list = tokens[mask].tolist()\n        \n        if not token_list:\n            # All nulls, return original\n            yield tokens\n            continue\n        \n        # Initialize output as copy of input\n        output = tokens.copy()\n        \n        # Process in sub-batches\n        start = 0\n        while start < len(token_list):\n            end = min(start + BATCH_SIZE, len(token_list))\n            batch_tokens = token_list[start:end]\n            \n            # Call Lambda API\n            resp = requests.post(\n                LAMBDA_URL,\n                json={{\"tokens\": batch_tokens}},\n                headers={{\n                    \"Content-Type\": \"application/json\",\n                    \"X-Skyflow-Operation\": \"detokenize\",\n                    \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                    \"X-Skyflow-Vault-ID\": VAULT_ID\n                }},\n                timeout=REQUEST_TIMEOUT\n            )\n            resp.raise_for_status()\n            \n            # Parse response and create token->value mapping\n            data = resp.json().get(\"data\", [])\n            token_to_value = {{r[\"token\"]: r[\"value\"] for r in data}}\n            \n            # Write back into output Series only where not null\n            sub_mask_idx = mask[mask].index[start:end]\n            for idx, tok in zip(sub_mask_idx, batch_tokens):\n                output.at[idx] = token_to_value.get(tok)\n            \n            start = end\n        \n        # Yield result as Series\n        yield output\n$$\n\"\"\")\n\nprint(\"âœ“ Created batch Python UDF: skyflow_detokenize(token)\")\nprint(f\"  Location: {CATALOG}.{SCHEMA}.skyflow_detokenize\")\nprint(f\"  Type: PARAMETER STYLE PANDAS (batched execution)\")\nprint(f\"  Batch Size: {BATCH_SIZE} rows per Lambda call\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint(\"\\nThis function is now:\")\nprint(\"  - Persistent in Unity Catalog\")\nprint(\"  - Shareable across workspaces/users\")\nprint(\"  - Usable in SQL queries and persistent views\")\nprint(\"  - Batched for optimal performance\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup Complete!\n\nTwo Unity Catalog Batch Python UDFs have been created:\n\n1. **skyflow_tokenize_column(column_value, column_name)** - Batched tokenization\n2. **skyflow_detokenize(token)** - Batched detokenization\n\nThese functions are now:\n- âœ… Persistent in Unity Catalog\n- âœ… Governed and shareable with proper access control\n- âœ… Callable from SQL queries\n- âœ… Usable in persistent views\n- âœ… Batched for optimal throughput (configurable batch size to Lambda)\n\n**Key Technology:** These functions use `PARAMETER STYLE PANDAS` which enables batched, vectorized execution while maintaining persistence and governance in Unity Catalog.\n\n---\n\n# Usage Examples\n\nThe cells below demonstrate how to use the batch Python UDFs."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "\n",
    "Create sample data for testing the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, current_timestamp\n",
    "\n",
    "# Configure number of test rows\n",
    "NUM_ROWS = 100\n",
    "\n",
    "# Create test data\n",
    "test_df = spark.range(NUM_ROWS).select(\n",
    "    (expr(\"id + 1\").alias(\"user_id\")),\n",
    "    expr(\"concat('user_', id)\").alias(\"username\"),\n",
    "    expr(\"concat('user', id, '@example.com')\").alias(\"email\"),\n",
    "    expr(\"concat('+1-555-', LPAD(id % 1000, 3, '0'), '-', LPAD((id * 7) % 10000, 4, '0'))\").alias(\"phone\"),\n",
    "    current_timestamp().alias(\"created_at\")\n",
    ")\n",
    "\n",
    "# Save as table\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.raw_users\")\n",
    "\n",
    "print(f\"âœ“ Created {CATALOG}.{SCHEMA}.raw_users table with {NUM_ROWS} rows\")\n",
    "display(spark.table(f\"{CATALOG}.{SCHEMA}.raw_users\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tokenize Data with Batch Python UDF\n\nUse the batch Python UDF to tokenize email addresses efficiently.\n\n**Note:** Due to Unity Catalog `PARAMETER STYLE PANDAS` limitations, you must use the **derived column pattern** when passing column names. The literal string `'email'` must be converted to a column expression using a subquery.\n\n### Pattern Explanation\n\n```sql\n-- Convert literal 'email' to a column\nSELECT\n    skyflow_tokenize_column(email, column_name) as email_token\nFROM (\n    SELECT email, 'email' AS column_name\n    FROM users\n) t\n```\n\nThe subquery creates a `column_name` column filled with `'email'`, which UC properly converts to a pandas Series in the UDF."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize using the batch Python UDF with derived column pattern\n# Using CTE to ensure column_name is a proper column expression\nresult = spark.sql(f\"\"\"\n    WITH prepared_data AS (\n        SELECT\n            user_id,\n            username,\n            email,\n            phone,\n            created_at,\n            'email' AS column_name\n        FROM {CATALOG}.{SCHEMA}.raw_users\n    )\n    SELECT \n        user_id,\n        username,\n        email,\n        {CATALOG}.{SCHEMA}.skyflow_tokenize_column(email, column_name) as email_token,\n        phone,\n        created_at\n    FROM prepared_data\n\"\"\")\n\n# Save tokenized data\nresult.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.tokenized_users\")\n\nprint(f\"âœ“ Created {CATALOG}.{SCHEMA}.tokenized_users table with tokenized emails\")\nprint(f\"  Tokenization was batched at {BATCH_SIZE} rows per Lambda API call\")\ndisplay(spark.table(f\"{CATALOG}.{SCHEMA}.tokenized_users\").limit(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Persistent Detokenized View\n",
    "\n",
    "Create a **persistent view** (not possible with temporary UDFs!) that automatically detokenizes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PERSISTENT view that detokenizes email tokens on-the-fly\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.users_detokenized AS\n",
    "    SELECT \n",
    "        user_id,\n",
    "        username,\n",
    "        {CATALOG}.{SCHEMA}.skyflow_detokenize(email_token) as email,\n",
    "        phone,\n",
    "        created_at\n",
    "    FROM {CATALOG}.{SCHEMA}.tokenized_users\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ“ Created PERSISTENT view: {CATALOG}.{SCHEMA}.users_detokenized\")\n",
    "print(\"\\nðŸŽ‰ Key Achievement: This is a PERSISTENT view using batched detokenization!\")\n",
    "print(\"   - Not possible with temporary Pandas UDFs\")\n",
    "print(\"   - Much more efficient than scalar UC UDFs\")\n",
    "print(f\"   - Batches at {BATCH_SIZE} rows per API call\")\n",
    "print(\"   - Accessible to all users with permissions\")\n",
    "print(\"\\nQuerying the view:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{SCHEMA}.users_detokenized LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Roundtrip Accuracy\n",
    "\n",
    "Compare original values with tokenized and detokenized values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs detokenized\n",
    "verification_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        t.user_id,\n",
    "        r.email as original_email,\n",
    "        t.email_token,\n",
    "        d.email as detokenized_email,\n",
    "        CASE\n",
    "            WHEN r.email = d.email THEN 'MATCH'\n",
    "            ELSE 'MISMATCH'\n",
    "        END as verification\n",
    "    FROM {CATALOG}.{SCHEMA}.tokenized_users t\n",
    "    JOIN {CATALOG}.{SCHEMA}.raw_users r ON t.user_id = r.user_id\n",
    "    JOIN {CATALOG}.{SCHEMA}.users_detokenized d ON t.user_id = d.user_id\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(verification_df)\n",
    "\n",
    "# Check for any mismatches\n",
    "mismatches = verification_df.filter(\"verification = 'MISMATCH'\").count()\n",
    "if mismatches == 0:\n",
    "    print(\"\\nâœ“ All records match! Tokenization â†’ Detokenization working correctly.\")\n",
    "    print(\"\\nðŸŽ‰ Batch Python UDFs are working perfectly with batched Lambda calls!\")\n",
    "else:\n",
    "    print(f\"\\nâœ— Found {mismatches} mismatches - investigate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grant Permissions (Optional)\n",
    "\n",
    "Control access to the batch Python UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Grant function execution permissions\n# Uncomment and adjust as needed\n\n# spark.sql(f\"GRANT EXECUTE ON FUNCTION {CATALOG}.{SCHEMA}.skyflow_tokenize_column TO `data_engineers`\")\n# spark.sql(f\"GRANT EXECUTE ON FUNCTION {CATALOG}.{SCHEMA}.skyflow_detokenize TO `data_engineers`\")\n# spark.sql(f\"GRANT SELECT ON VIEW {CATALOG}.{SCHEMA}.users_detokenized TO `analysts`\")\n\nprint(\"ðŸ’¡ Tip: Use Unity Catalog's access control to govern who can:\")\nprint(\"   - Execute tokenization (potentially create PII tokens)\")\nprint(\"   - Execute detokenization (access plaintext PII)\")\nprint(\"   - Query detokenized views (read PII)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "To remove all test resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment to drop test tables, views, and functions\n# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.raw_users\")\n# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.tokenized_users\")\n# spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{SCHEMA}.users_detokenized\")\n# spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{SCHEMA}.skyflow_detokenize\")\n# spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{SCHEMA}.skyflow_tokenize_column\")\n# print(\"âœ“ Cleanup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated **Unity Catalog Batch Python UDFs** for Skyflow integration in Databricks.\n\n### Key Achievements\n\nâœ… **Batched Execution**\n- Configurable batch size to Lambda (default 500 rows per call for high throughput)\n- Lambda internally batches at 25 rows per Skyflow API call\n- Dramatically reduces API costs compared to row-by-row processing\n\nâœ… **Persistent & Governed**\n- Functions stored in Unity Catalog\n- Shareable across workspaces and users\n- Fine-grained access control\n\nâœ… **Production Ready**\n- Usable in persistent views\n- Callable from SQL queries\n- Survives cluster restarts\n- Perfect for BI tools and team collaboration\n\n### The Magic: PARAMETER STYLE PANDAS\n\n```sql\nCREATE FUNCTION ... \nLANGUAGE PYTHON\nPARAMETER STYLE PANDAS  -- This enables batched execution!\nHANDLER 'handler_function'\n```\n\nThis directive tells Databricks to:\n1. Group rows into batches (pandas Series)\n2. Call the handler function with batches (not individual rows)\n3. Process results as batches\n\n### Handler Signature Patterns\n\n**Single-argument UDF** (e.g., detokenization):\n```python\ndef handler(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n    for values in batch_iter:\n        # Process batch\n        yield pd.Series(results)\n```\n\n**Two-argument UDF** (e.g., tokenization):\n```python\ndef handler(batch_iter: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n    for arg1, arg2 in batch_iter:\n        # Process batch\n        yield pd.Series(results)\n```\n\n### Performance Notes\n\n- **Batch Size to Lambda:** Default 500 rows per call (configurable via BATCH_SIZE parameter)\n- **Lambda to Skyflow:** Automatic internal batching at 25 rows per Skyflow API call\n- **Timeout:** 30 seconds (must be â‰¤ Lambda timeout)\n- **Parallelization:** Spark automatically parallelizes across partitions\n- **Scalability:** High batch size to Lambda allows Lambda to scale out and churn through data quickly\n\nFor 100K rows with BATCH_SIZE=500:\n- ~200 Lambda calls\n- Lambda then makes ~4,000 Skyflow API calls (at 25 rows each)\n- Compare to row-by-row scalar UDFs: 100,000 Lambda calls (500x more expensive!)\n\n### Derived Column Pattern\n\nDue to Unity Catalog limitations, you cannot pass literal strings directly to `PARAMETER STYLE PANDAS` functions. Use the derived column pattern:\n\n```sql\n-- Convert literal to column\nWITH prepared AS (\n  SELECT email, 'email' AS email_col\n  FROM users\n)\nSELECT skyflow_tokenize_column(email, email_col)\nFROM prepared\n```\n\n### Next Steps\n\n1. **Monitor Performance:** Check Lambda CloudWatch logs for batch sizes and timing\n2. **Tune Batch Size:** Adjust BATCH_SIZE based on your data size, network latency, and Lambda timeout\n3. **Set Up Governance:** Grant appropriate permissions to users/groups\n4. **Create Views:** Build persistent views for your BI tools and analysts"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}