{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skyflow Temporary UDFs - Complete Setup & Usage\n",
    "\n",
    "This notebook demonstrates tokenization and detokenization using **temporary Pandas UDFs** with optimal API batching.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Configure cell 1** with your credentials\n",
    "2. **Run cells 1-3** to register the UDFs\n",
    "3. **UDFs are registered** and ready to use immediately!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- ‚úÖ Databricks cluster (any configuration)\n",
    "- ‚úÖ Lambda API deployed and accessible from Databricks\n",
    "- ‚úÖ Skyflow credentials (Cluster ID, Vault ID, Table name)\n",
    "\n",
    "## Key Differences from Unity Catalog Approach\n",
    "\n",
    "| Feature | Temporary UDFs | Unity Catalog Functions |\n",
    "|---------|----------------|-------------------------|\n",
    "| **Setup** | Run notebook cells | Create functions once |\n",
    "| **Lifecycle** | Session-scoped | Permanent (cluster-wide) |\n",
    "| **Availability** | Single session | All users with permission |\n",
    "| **Configuration** | In notebook | Embedded in functions |\n",
    "| **API Batching** | ‚úÖ Yes (25 rows/call) | ‚ùå No (1 row/call - scalar) |\n",
    "| **Performance** | Optimal for high-volume | Spark parallelizes |\n",
    "| **Views** | Must be temporary | Can be persistent |\n",
    "| **Use Case** | High-volume, development | Shared access, governance |\n",
    "\n",
    "## Performance Advantage\n",
    "\n",
    "Pandas UDFs batch API calls for optimal performance:\n",
    "\n",
    "```python\n",
    "# Temporary UDF approach (this notebook)\n",
    "# 100 values = 4 API calls (25 values per call)\n",
    "skyflow_tokenize(column)  \n",
    "\n",
    "# Unity Catalog approach (databricks_unity_catalog.ipynb)\n",
    "# 100 values = 100 API calls (1 value per call)\n",
    "skyflow_tokenize(value, table_name, column_name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Setup: Register UDFs\n",
    "\n",
    "Run cells 1-3 to register the temporary UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 1: Configuration - UPDATE THESE VALUES\n# ============================================================================\n\n# Lambda API configuration\nLAMBDA_URL = \"https://YOUR_API_ID.execute-api.YOUR_REGION.amazonaws.com/processDatabricks\"\n\n# Skyflow configuration\nCLUSTER_ID = \"YOUR_CLUSTER_ID\"\nVAULT_ID = \"YOUR_VAULT_ID\"\nTABLE = \"TABLE_NAME\"\n\n# Performance tuning\nBATCH_SIZE = 1000  # Rows per Lambda call (Lambda batches internally at 25 rows/Skyflow API call)\nREQUEST_TIMEOUT = 30  # Timeout in seconds (must be <= Lambda timeout, default 30s)\n\nprint(\"=\" * 60)\nprint(\"Configuration Summary\")\nprint(\"=\" * 60)\nprint(f\"Lambda URL:  {LAMBDA_URL}\")\nprint(f\"Cluster ID:  {CLUSTER_ID}\")\nprint(f\"Vault ID:    {VAULT_ID}\")\nprint(f\"Table:       {TABLE}\")\nprint(f\"Batch Size:  {BATCH_SIZE}\")\nprint(f\"Timeout:     {REQUEST_TIMEOUT}s\")\nprint(\"=\" * 60)\nprint(\"\\n‚úì Configuration loaded\")\nprint(\"\\nNext: Run cells 2-3 to register UDFs\")\nprint(\"\")\nprint(\"üí° Performance Tips:\")\nprint(\"   - BATCH_SIZE: Number of rows sent to Lambda per request\")\nprint(\"   - Lambda internally batches at 25 rows per Skyflow API call\")\nprint(\"   - Reduce BATCH_SIZE if you see timeout errors (e.g., 500, 250)\")\nprint(\"   - Increase REQUEST_TIMEOUT if Lambda needs more processing time\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 2: Register Tokenization UDF\n# ============================================================================\n#\n# NOTE: This Pandas UDF accepts column_name as a runtime parameter.\n# Pass the column name using lit() when calling the function.\n#\n# Performance characteristics:\n# - Batches BATCH_SIZE rows per Lambda call (default: 1000)\n# - Lambda further batches at 25 rows per Skyflow API call\n# - Significantly reduces API costs and latency vs scalar functions\n# - Session-scoped only (must re-register after cluster restart)\n#\n# For persistent functions with governance, use databricks_unity_catalog.ipynb instead.\n#\n\nimport pandas as pd\nimport requests\nfrom pyspark.sql.functions import pandas_udf, lit\n\n@pandas_udf(\"string\")\ndef skyflow_tokenize(values: pd.Series, column_names: pd.Series) -> pd.Series:\n    \"\"\"\n    Tokenize a column of sensitive data using the Lambda API with batching.\n\n    Args:\n        values: Pandas Series containing plaintext values to tokenize (may include NULLs)\n        column_names: Pandas Series containing the column name (same for all rows via lit())\n\n    Returns:\n        Pandas Series containing Skyflow tokens\n        \n    Usage:\n        df.withColumn(\"email_token\", skyflow_tokenize(col(\"email\"), lit(\"email\")))\n    \"\"\"\n    # Extract column name (same for all rows)\n    column_name = column_names.iloc[0]\n    \n    results = [None] * len(values)\n\n    # Process in batches to optimize API calls\n    for start in range(0, len(values), BATCH_SIZE):\n        end = min(start + BATCH_SIZE, len(values))\n        batch = values.iloc[start:end].tolist()\n\n        # Filter out NULL values and build records\n        records = []\n        indices = []\n        for i in range(start, end):\n            val = values.iloc[i]\n            if val is not None:\n                records.append({column_name: val})\n                indices.append(i)\n\n        if not records:\n            continue\n\n        # Call Lambda API\n        resp = requests.post(\n            LAMBDA_URL,\n            json={\"records\": records},\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"X-Skyflow-Operation\": \"tokenize\",\n                \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                \"X-Skyflow-Vault-ID\": VAULT_ID,\n                \"X-Skyflow-Table\": TABLE\n            },\n            timeout=REQUEST_TIMEOUT\n        )\n        resp.raise_for_status()\n\n        # Parse response and extract tokens\n        data = resp.json().get(\"data\", [])\n\n        # Fill results for this batch\n        for idx, record in enumerate(data):\n            result_index = indices[idx]\n            results[result_index] = record.get(column_name)\n\n    return pd.Series(results)\n\n# Register the UDF for SQL use\nspark.udf.register(\"skyflow_tokenize\", skyflow_tokenize)\n\nprint(\"‚úì Created function: skyflow_tokenize(value, column_name)\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint(f\"  Batch Size: {BATCH_SIZE} rows per Lambda call\")\nprint(f\"  Timeout: {REQUEST_TIMEOUT}s\")\nprint(\"\")\nprint(\"Usage:\")\nprint(\"  # Python DataFrame API\")\nprint(\"  df.withColumn('email_token', skyflow_tokenize(col('email'), lit('email')))\")\nprint(\"  \")\nprint(\"  # SQL (after registering)\")\nprint(\"  SELECT skyflow_tokenize(email, 'email') as token FROM users\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 3: Register Detokenization UDF\n# ============================================================================\n#\n# NOTE: This is also a Pandas UDF with 2-level batching.\n# Lambda internally batches at 25 tokens per Skyflow API call.\n#\n\n@pandas_udf(\"string\")\ndef skyflow_detokenize(tokens: pd.Series) -> pd.Series:\n    \"\"\"\n    Detokenize a column of Skyflow tokens using the Lambda API with batching.\n\n    Args:\n        tokens: Pandas Series containing Skyflow tokens (may include NULLs)\n\n    Returns:\n        Pandas Series containing detokenized values\n    \"\"\"\n    results = [None] * len(tokens)\n\n    # Process in batches to optimize API calls\n    for start in range(0, len(tokens), BATCH_SIZE):\n        end = min(start + BATCH_SIZE, len(tokens))\n        batch = tokens.iloc[start:end].tolist()\n\n        # Filter out NULL values\n        non_null = [t for t in batch if t is not None]\n        if not non_null:\n            continue\n\n        # Call Lambda API\n        resp = requests.post(\n            LAMBDA_URL,\n            json={\"tokens\": non_null},\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"X-Skyflow-Operation\": \"detokenize\",\n                \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                \"X-Skyflow-Vault-ID\": VAULT_ID\n            },\n            timeout=REQUEST_TIMEOUT\n        )\n        resp.raise_for_status()\n\n        # Parse response and map tokens to values\n        data = resp.json().get(\"data\", [])\n        token_to_value = {r[\"token\"]: r[\"value\"] for r in data}\n\n        # Fill results for this batch\n        for i in range(start, end):\n            tok = tokens.iloc[i]\n            results[i] = None if tok is None else token_to_value.get(tok)\n\n    return pd.Series(results)\n\n# Register the UDF for SQL use\nspark.udf.register(\"skyflow_detokenize\", skyflow_detokenize)\n\nprint(\"‚úì Created function: skyflow_detokenize(token)\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint(f\"  Cluster ID: {CLUSTER_ID}\")\nprint(f\"  Vault ID: {VAULT_ID}\")\nprint(f\"  Batch Size: {BATCH_SIZE} tokens per Lambda call\")\nprint(f\"  Timeout: {REQUEST_TIMEOUT}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Complete!\n",
    "\n",
    "The following temporary UDFs have been registered:\n",
    "\n",
    "1. **skyflow_tokenize(value)** - Tokenizes sensitive data with batching\n",
    "2. **skyflow_detokenize(token)** - Detokenizes tokens back to plaintext with batching\n",
    "\n",
    "These functions are now:\n",
    "- Available in your current Spark session\n",
    "- Usable in SQL queries and DataFrame operations\n",
    "- Optimized with API batching (25 rows per call)\n",
    "\n",
    "**Note:** UDFs must be re-registered after cluster restart.\n",
    "\n",
    "---\n",
    "\n",
    "# Usage Examples\n",
    "\n",
    "The cells below demonstrate how to use the UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "\n",
    "Create sample data for testing the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, current_timestamp, col\n",
    "\n",
    "# Configure number of test rows\n",
    "NUM_ROWS = 100\n",
    "\n",
    "# Create test data\n",
    "test_df = spark.range(NUM_ROWS).select(\n",
    "    (expr(\"id + 1\").alias(\"user_id\")),\n",
    "    expr(\"concat('user_', id)\").alias(\"username\"),\n",
    "    expr(\"concat('user', id, '@example.com')\").alias(\"email\"),\n",
    "    expr(\"concat('555-01-', LPAD(id % 10000, 4, '0'))\").alias(\"ssn\"),\n",
    "    expr(\"concat('+1-555-', LPAD(id % 1000, 3, '0'), '-', LPAD((id * 7) % 10000, 4, '0'))\").alias(\"phone\"),\n",
    "    current_timestamp().alias(\"created_at\")\n",
    ")\n",
    "\n",
    "# Save as table\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(\"raw_users\")\n",
    "\n",
    "print(f\"‚úì Created raw_users table with {NUM_ROWS} rows\")\n",
    "display(spark.table(\"raw_users\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Single Column\n",
    "\n",
    "Test the tokenization function on sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize email column - pass column name as runtime parameter\ndf = spark.table(\"raw_users\")\ndf_tokenized = df.withColumn(\"email_token\", skyflow_tokenize(col(\"email\"), lit(\"email\")))\n\n# Display results\ndisplay(df_tokenized.select(\"user_id\", \"username\", \"email\", \"email_token\", \"phone\").limit(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenized Table\n",
    "\n",
    "Save tokenized data to a new table (keeping non-sensitive columns as plaintext):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a new table with tokenized sensitive columns\ntokenized_df = spark.table(\"raw_users\").select(\n    col(\"user_id\"),\n    col(\"username\"),\n    skyflow_tokenize(col(\"email\"), lit(\"email\")).alias(\"email_token\"),\n    col(\"phone\"),  # Non-sensitive: keep as plaintext\n    col(\"created_at\")\n)\n\n# Save tokenized data\ntokenized_df.write.mode(\"overwrite\").saveAsTable(\"tokenized_users\")\n\ncount = spark.table(\"tokenized_users\").count()\nprint(f\"‚úì Created tokenized_users table with {count} rows\")\ndisplay(spark.table(\"tokenized_users\").limit(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detokenize Tokens\n",
    "\n",
    "Test the detokenization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize the tokenized_users table\n",
    "df_tokens = spark.table(\"tokenized_users\")\n",
    "df_detokenized = df_tokens.withColumn(\"email_detokenized\", skyflow_detokenize(col(\"email_token\")))\n",
    "\n",
    "# Display: token vs detokenized value\n",
    "display(df_detokenized.select(\"user_id\", \"username\", \"email_token\", \"email_detokenized\").limit(10))\n",
    "\n",
    "print(\"\\n‚úì Roundtrip complete: Tokens ‚Üí Detokenized values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Temporary Detokenized View\n",
    "\n",
    "Create a temporary view that automatically detokenizes tokens for authorized queries:\n",
    "\n",
    "**Note:** The view must be temporary because the UDFs are session-scoped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view that detokenizes email tokens on-the-fly\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW users_detokenized AS\n",
    "    SELECT \n",
    "        user_id,\n",
    "        username,\n",
    "        skyflow_detokenize(email_token) as email,\n",
    "        phone,\n",
    "        created_at\n",
    "    FROM tokenized_users\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created TEMPORARY view: users_detokenized\")\n",
    "print(\"  This view exists for the duration of your Spark session\")\n",
    "display(spark.sql(\"SELECT * FROM users_detokenized LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Roundtrip Accuracy\n",
    "\n",
    "Compare original values with tokenized and detokenized values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs detokenized\n",
    "verification_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        t.user_id,\n",
    "        r.email as original_email,\n",
    "        t.email_token,\n",
    "        d.email as detokenized_email,\n",
    "        CASE\n",
    "            WHEN r.email = d.email THEN 'MATCH'\n",
    "            ELSE 'MISMATCH'\n",
    "        END as verification\n",
    "    FROM tokenized_users t\n",
    "    JOIN raw_users r ON t.user_id = r.user_id\n",
    "    JOIN users_detokenized d ON t.user_id = d.user_id\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(verification_df)\n",
    "\n",
    "# Check for any mismatches\n",
    "mismatches = verification_df.filter(\"verification = 'MISMATCH'\").count()\n",
    "if mismatches == 0:\n",
    "    print(\"\\n‚úì All records match! Tokenization ‚Üí Detokenization working correctly.\")\n",
    "else:\n",
    "    print(f\"\\n‚úó Found {mismatches} mismatches - investigate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Usage Examples\n",
    "\n",
    "Both UDFs can be used directly in SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Tokenize with SQL - pass column name as string literal\nresult = spark.sql(\"\"\"\n    SELECT \n        user_id,\n        email,\n        skyflow_tokenize(email, 'email') as email_token\n    FROM raw_users\n    WHERE user_id <= 5\n\"\"\")\ndisplay(result)\n\n# Example 2: Detokenize with filtering\nresult = spark.sql(\"\"\"\n    SELECT \n        user_id,\n        skyflow_detokenize(email_token) as email\n    FROM tokenized_users\n    WHERE phone LIKE '+1-555-000%'\n    LIMIT 10\n\"\"\")\ndisplay(result)\n\n# Example 3: Create table with inline tokenization\nspark.sql(\"\"\"\n    CREATE OR REPLACE TABLE secure_contacts AS\n    SELECT \n        user_id,\n        username,\n        skyflow_tokenize(email, 'email') as email_token,\n        phone\n    FROM raw_users\n\"\"\")\nprint(\"\\n‚úì Created secure_contacts table with tokenized emails\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "To remove all test resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to drop test tables and views\n",
    "# spark.sql(\"DROP TABLE IF EXISTS raw_users\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS tokenized_users\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS secure_contacts\")\n",
    "# spark.sql(\"DROP VIEW IF EXISTS users_detokenized\")\n",
    "# print(\"‚úì Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated temporary Pandas UDFs for Skyflow integration:\n",
    "\n",
    "**Key Features:**\n",
    "- Simple setup - Register UDFs in your session (cells 1-3)\n",
    "- Optimal batching - 25 rows per API call (configurable)\n",
    "- Session-scoped - Functions available for session duration\n",
    "- High performance - Significantly fewer API calls than scalar functions\n",
    "- Temporary views - Create session-scoped detokenized views\n",
    "- Flexible configuration - All settings in notebook (not embedded)\n",
    "\n",
    "**Important Performance Note:**\n",
    "\n",
    "Pandas UDFs with batching provide **optimal API efficiency**:\n",
    "- Makes 1 API call per 25 rows (vs 1 call per row for Unity Catalog)\n",
    "- Reduces API costs by ~25x compared to scalar functions\n",
    "- Best for high-volume tokenization workloads\n",
    "\n",
    "**When to use Temporary UDFs vs Unity Catalog:**\n",
    "\n",
    "| Use Case | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| High-volume tokenization (100K+ rows) | Temporary Pandas UDFs (this notebook) |\n",
    "| Ad-hoc analysis, development | Temporary Pandas UDFs (this notebook) |\n",
    "| Optimal API call batching required | Temporary Pandas UDFs (this notebook) |\n",
    "| Production pipelines, shared resources | Unity Catalog (databricks_unity_catalog.ipynb) |\n",
    "| Persistent views required | Unity Catalog (databricks_unity_catalog.ipynb) |\n",
    "| Team collaboration, governance | Unity Catalog (databricks_unity_catalog.ipynb) |\n",
    "\n",
    "**To update configuration:**\n",
    "1. Update variables in cell 1\n",
    "2. Re-run cells 2-3 to re-register UDFs with new configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}