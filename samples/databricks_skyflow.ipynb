{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "# Skyflow Tokenization & Detokenization for Databricks\n\nThis notebook demonstrates how to tokenize and detokenize sensitive data in Databricks using Pandas UDFs that call the Skyflow Lambda API.\n\n## Setup Instructions\n1. Update the configuration values in the next cell\n2. Run all cells to register both UDFs\n3. The notebook includes a complete example:\n   - Generate test data with sensitive information\n   - Tokenize sensitive columns\n   - Detokenize tokens to verify roundtrip\n\n## Features\n- Batch processing (10,000 values per API call by default)\n- Handles NULL values gracefully\n- Configurable timeout and batch size\n- Efficient distributed processing with Pandas UDF\n- Complete tokenization → detokenization roundtrip example"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - Update these values for your environment\n",
    "LAMBDA_URL  = \"LAMBDA_URL/processDatabricks\"\n",
    "CLUSTER_ID  = \"CLUSTER_ID\"\n",
    "VAULT_ID    = \"VAULT_ID\"\n",
    "TABLE       = \"TABLE_NAME\"\n",
    "COLUMN_NAME = \"COLUMN_NAME\"\n",
    "BATCH_SIZE  = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e5f6a7-b8c9-0123-def0-234567890123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def skyflow_tokenize(values: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Tokenize a column of sensitive data using the Lambda API.\n",
    "\n",
    "    Args:\n",
    "        values: Pandas Series containing plaintext values to tokenize (may include NULLs)\n",
    "\n",
    "    Returns:\n",
    "        Pandas Series containing Skyflow tokens\n",
    "    \"\"\"\n",
    "    results = [None] * len(values)\n",
    "\n",
    "    # Process in batches to avoid overwhelming the API\n",
    "    for start in range(0, len(values), BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, len(values))\n",
    "        batch = values.iloc[start:end].tolist()\n",
    "\n",
    "        # Filter out NULL values and build records\n",
    "        records = []\n",
    "        indices = []\n",
    "        for i in range(start, end):\n",
    "            val = values.iloc[i]\n",
    "            if val is not None:\n",
    "                records.append({COLUMN_NAME: val})\n",
    "                indices.append(i)\n",
    "\n",
    "        if not records:\n",
    "            continue\n",
    "\n",
    "        # Call Lambda API\n",
    "        resp = requests.post(\n",
    "            LAMBDA_URL,\n",
    "            json={\"records\": records},\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"X-Skyflow-Operation\": \"tokenize\",\n",
    "                \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n",
    "                \"X-Skyflow-Vault-ID\": VAULT_ID,\n",
    "                \"X-Skyflow-Table\": TABLE\n",
    "            },\n",
    "            timeout=10\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        # Parse response and extract tokens\n",
    "        data = resp.json().get(\"data\", [])\n",
    "\n",
    "        # Fill results for this batch\n",
    "        for idx, record in enumerate(data):\n",
    "            result_index = indices[idx]\n",
    "            results[result_index] = record.get(COLUMN_NAME)\n",
    "\n",
    "    return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f6a7b8-c9d0-1234-ef01-345678901234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the UDF for SQL use\n",
    "spark.udf.register(\"skyflow_tokenize\", skyflow_tokenize)\n",
    "\n",
    "print(\"✅ Skyflow tokenization UDF registered successfully!\")\n",
    "print(f\"   Lambda URL: {LAMBDA_URL}\")\n",
    "print(f\"   Cluster ID: {CLUSTER_ID}\")\n",
    "print(f\"   Vault ID: {VAULT_ID}\")\n",
    "print(f\"   Table: {TABLE}\")\n",
    "print(f\"   Column: {COLUMN_NAME}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  SELECT skyflow_tokenize(sensitive_column) as token FROM my_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Generate Test Data\n\nRun this cell to create a sample `raw_users` table for testing:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pyspark.sql.functions import expr, current_timestamp\n\n# Configure number of test rows\nNUM_ROWS = 100\n\n# Create test data with auto-generated rows\ntest_df = spark.range(NUM_ROWS).select(\n    (expr(\"id + 1\").alias(\"user_id\")),\n    expr(\"concat('user_', id)\").alias(\"username\"),\n    expr(\"concat('user', id, '@example.com')\").alias(\"email\"),\n    expr(\"concat('555-01-', LPAD(id % 10000, 4, '0'))\").alias(\"ssn\"),\n    expr(\"concat('+1-555-', LPAD(id % 1000, 3, '0'), '-', LPAD((id * 7) % 10000, 4, '0'))\").alias(\"phone\"),\n    current_timestamp().alias(\"created_at\")\n)\n\n# Save as table\ntest_df.write.mode(\"overwrite\").saveAsTable(\"raw_users\")\n\nprint(f\"✅ Created raw_users table with {NUM_ROWS} rows\")\ndisplay(spark.table(\"raw_users\").limit(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Tokenize Single Column",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pyspark.sql.functions import col\n\n# Tokenize email column only\ndf = spark.table(\"raw_users\")\ndf_tokenized = df.withColumn(\"email_token\", skyflow_tokenize(col(\"email\")))\n\n# Display results\ndisplay(df_tokenized.select(\"user_id\", \"username\", \"email\", \"email_token\", \"phone\").limit(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create Tokenized Table\n\nSave tokenized data to a new table (keeping non-sensitive columns as plaintext):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a new table with tokenized sensitive columns\ntokenized_df = spark.table(\"raw_users\").select(\n    col(\"user_id\"),\n    col(\"username\"),\n    skyflow_tokenize(col(\"email\")).alias(\"email_token\"),\n    col(\"phone\"),  # Non-sensitive: keep as plaintext\n    col(\"created_at\")\n)\n\n# Save tokenized data\ntokenized_df.write.mode(\"overwrite\").saveAsTable(\"tokenized_users\")\n\nprint(f\"✅ Created tokenized_users table with {tokenized_df.count()} rows\")\ndisplay(spark.table(\"tokenized_users\").limit(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Detokenization\n\nNow let's register the detokenization UDF to convert tokens back to plaintext:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@pandas_udf(\"string\")\ndef skyflow_detokenize(tokens: pd.Series) -> pd.Series:\n    \"\"\"\n    Detokenize a column of Skyflow tokens using the Lambda API.\n\n    Args:\n        tokens: Pandas Series containing Skyflow tokens (may include NULLs)\n\n    Returns:\n        Pandas Series containing decrypted values\n    \"\"\"\n    results = [None] * len(tokens)\n\n    # Process in batches to avoid overwhelming the API\n    for start in range(0, len(tokens), BATCH_SIZE):\n        end = min(start + BATCH_SIZE, len(tokens))\n        batch = tokens.iloc[start:end].tolist()\n\n        # Filter out NULL values\n        non_null = [t for t in batch if t is not None]\n        if not non_null:\n            continue\n\n        # Call Lambda API\n        resp = requests.post(\n            LAMBDA_URL,\n            json={\"tokens\": non_null},\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"X-Skyflow-Operation\": \"detokenize\",\n                \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n                \"X-Skyflow-Vault-ID\": VAULT_ID\n            },\n            timeout=10\n        )\n        resp.raise_for_status()\n\n        # Parse response and map tokens to values\n        data = resp.json().get(\"data\", [])\n        token_to_value = {r[\"token\"]: r[\"value\"] for r in data}\n\n        # Fill results for this batch\n        for i in range(start, end):\n            tok = tokens.iloc[i]\n            results[i] = None if tok is None else token_to_value.get(tok)\n\n    return pd.Series(results)\n\n# Register the UDF for SQL use\nspark.udf.register(\"skyflow_detokenize\", skyflow_detokenize)\n\nprint(\"✅ Skyflow detokenization UDF registered successfully!\")\nprint(\"\\nUsage:\")\nprint(\"  SELECT skyflow_detokenize(token_column) as value FROM my_table\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detokenize the Tokenized Table\n\nNow let's detokenize the tokens we created to verify the roundtrip:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detokenize the tokenized_users table\ndf_tokens = spark.table(\"tokenized_users\")\ndf_decrypted = df_tokens.withColumn(\"email_decrypted\", skyflow_detokenize(col(\"email_token\")))\n\n# Display: token vs decrypted value\ndisplay(df_decrypted.select(\"user_id\", \"username\", \"email_token\", \"email_decrypted\").limit(10))\n\nprint(\"\\n✅ Roundtrip complete! Tokens → Decrypted values\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create Detokenized View\n\nCreate a temporary view that automatically detokenizes tokens for authorized queries:\n\n**Note:** The view must be temporary because the UDFs are registered as temporary functions. The view will exist for the duration of your Spark session.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a temporary view that detokenizes email tokens on-the-fly\nspark.sql(\"\"\"\n    CREATE OR REPLACE TEMP VIEW users_decrypted AS\n    SELECT \n        user_id,\n        username,\n        skyflow_detokenize(email_token) as email,\n        phone,\n        created_at\n    FROM tokenized_users\n\"\"\")\n\nprint(\"✅ Created users_decrypted temporary view\")\nprint(\"\\nQuery the view to see decrypted emails:\")\ndisplay(spark.sql(\"SELECT * FROM users_decrypted LIMIT 10\"))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## SQL Usage Examples\n\nBoth UDFs can be used directly in SQL queries:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Tokenize with SQL",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Tokenize a literal value\nresult = spark.sql(\"SELECT skyflow_tokenize('test@example.com') as token\")\ndisplay(result)\n\n# Example 2: Tokenize multiple rows\nresult = spark.sql(\"\"\"\n    SELECT \n        user_id,\n        email,\n        skyflow_tokenize(email) as email_token\n    FROM raw_users\n    WHERE user_id <= 5\n\"\"\")\ndisplay(result)\n\n# Example 3: Insert tokenized data directly\nspark.sql(\"\"\"\n    CREATE OR REPLACE TABLE secure_contacts AS\n    SELECT \n        user_id,\n        username,\n        skyflow_tokenize(email) as email_token,\n        phone\n    FROM raw_users\n\"\"\")\nprint(\"✅ Created secure_contacts table with tokenized emails\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Detokenize with SQL",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Detokenize from the view\nresult = spark.sql(\"\"\"\n    SELECT \n        user_id,\n        username,\n        email\n    FROM users_decrypted\n    WHERE user_id <= 5\n\"\"\")\ndisplay(result)\n\n# Example 2: Detokenize with filtering\nresult = spark.sql(\"\"\"\n    SELECT \n        user_id,\n        skyflow_detokenize(email_token) as email\n    FROM tokenized_users\n    WHERE phone LIKE '+1-555-000%'\n    LIMIT 10\n\"\"\")\ndisplay(result)\n\n# Example 3: Join tokenized and raw data\nresult = spark.sql(\"\"\"\n    SELECT \n        t.user_id,\n        t.username,\n        skyflow_detokenize(t.email_token) as decrypted_email,\n        r.email as original_email,\n        CASE \n            WHEN skyflow_detokenize(t.email_token) = r.email THEN '✅ Match'\n            ELSE '❌ Mismatch'\n        END as verification\n    FROM tokenized_users t\n    JOIN raw_users r ON t.user_id = r.user_id\n    LIMIT 10\n\"\"\")\ndisplay(result)\nprint(\"\\n✅ All SQL examples complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Query the Detokenized View\n\nThe view acts like a normal table - Skyflow detokenization happens transparently:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Simple SELECT\nresult = spark.sql(\"SELECT * FROM users_decrypted LIMIT 5\")\ndisplay(result)\n\n# Example 2: Filter by email domain (works on decrypted values!)\nresult = spark.sql(\"\"\"\n    SELECT \n        user_id,\n        username,\n        email\n    FROM users_decrypted\n    WHERE email LIKE '%@example.com'\n    LIMIT 10\n\"\"\")\ndisplay(result)\n\n# Example 3: Aggregate queries\nresult = spark.sql(\"\"\"\n    SELECT \n        SUBSTRING(email, LOCATE('@', email) + 1) as domain,\n        COUNT(*) as user_count\n    FROM users_decrypted\n    GROUP BY domain\n\"\"\")\ndisplay(result)\n\n# Example 4: Use in Python DataFrame API\ndf_decrypted = spark.table(\"users_decrypted\")\nresult = df_decrypted.filter(col(\"user_id\") <= 10).select(\"user_id\", \"email\")\ndisplay(result)\n\nprint(\"\\n✅ View queries complete! The view provides transparent access to decrypted data.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_skyflow_tokenize",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}