{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skyflow Unity Catalog External Functions - Complete Setup & Usage\n",
    "\n",
    "This notebook creates and demonstrates **persistent** Skyflow external functions in Unity Catalog.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Configure cell 1** with your credentials\n",
    "2. **Run cells 1-3** to create the functions\n",
    "3. **Functions are created** and ready to use immediately!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- ✅ Unity Catalog enabled in your workspace\n",
    "- ✅ CREATE FUNCTION privilege on the target catalog/schema\n",
    "- ✅ Cluster with Unity Catalog access\n",
    "- ✅ Lambda API deployed and accessible from Databricks\n",
    "\n",
    "## Key Differences from Temporary UDF Approach\n",
    "\n",
    "| Feature | Temporary UDFs | Unity Catalog Functions |\n",
    "|---------|----------------|-------------------------|\n",
    "| **Setup** | Run notebook cells | Create functions once |\n",
    "| **Lifecycle** | Session-scoped | Permanent (cluster-wide) |\n",
    "| **Availability** | Single session | All users with permission |\n",
    "| **Configuration** | In notebook | Embedded in functions |\n",
    "| **Cluster/Vault** | Pass as headers | Hardcoded at creation |\n",
    "| **Views** | Must be temporary | Can be persistent |\n",
    "| **Use Case** | Ad-hoc analysis | Production workloads |\n",
    "\n",
    "## Simplified Function Signatures\n",
    "\n",
    "Cluster_id and vault_id are **embedded in the function definitions**:\n",
    "\n",
    "```python\n",
    "# Temporary UDF approach (databricks_skyflow.ipynb)\n",
    "skyflow_tokenize(value)  # Cluster/vault in request headers\n",
    "\n",
    "# Unity Catalog approach (this notebook)\n",
    "skyflow_tokenize(value, table_name, column_name)  # Cluster/vault embedded\n",
    "skyflow_detokenize(token)  # Cluster/vault embedded\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Setup: Create External Functions\n",
    "\n",
    "Run cells 1-3 to create the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 1: Configuration - UPDATE THESE VALUES\n# ============================================================================\n\n# Unity Catalog configuration\nCATALOG = \"your_catalog_name\"\nSCHEMA = \"your_schema_name\"\n\n# Lambda API configuration\nLAMBDA_URL = \"https://YOUR_API_ID.execute-api.YOUR_REGION.amazonaws.com/processDatabricks\"\n\n# Skyflow configuration (embedded in functions)\nCLUSTER_ID = \"YOUR_CLUSTER_ID\"\nVAULT_ID = \"YOUR_VAULT_ID\"\n\n# Skyflow table name for tokenization examples\nTABLE = \"TABLE_NAME\"\n\n# Performance tuning\nREQUEST_TIMEOUT = 30  # Timeout in seconds (must be <= Lambda timeout, default 30s)\n\n# Set default catalog and schema\nspark.sql(f\"USE CATALOG {CATALOG}\")\nspark.sql(f\"USE SCHEMA {SCHEMA}\")\n\nprint(\"=\" * 60)\nprint(\"Configuration Summary\")\nprint(\"=\" * 60)\nprint(f\"Catalog:     {CATALOG}\")\nprint(f\"Schema:      {SCHEMA}\")\nprint(f\"Lambda URL:  {LAMBDA_URL}\")\nprint(f\"Cluster ID:  {CLUSTER_ID}\")\nprint(f\"Vault ID:    {VAULT_ID}\")\nprint(f\"Table:       {TABLE}\")\nprint(f\"Timeout:     {REQUEST_TIMEOUT}s\")\nprint(\"=\" * 60)\nprint(\"\\n✓ Configuration loaded\")\nprint(\"\\nNext: Run cells 2-3 to create functions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 2: Create Tokenization External Function\n# ============================================================================\n#\n# NOTE: Unity Catalog SQL UDFs are SCALAR functions that process one row at a time.\n# Unlike Pandas UDFs (used in databricks_skyflow.ipynb), these functions call the\n# Lambda API for each individual value.\n#\n# Performance characteristics:\n# - Pro: Spark handles parallelism across partitions automatically\n# - Pro: Functions are persistent and available cluster-wide\n# - Con: More API calls than batched Pandas UDFs (1 call per row vs batched calls)\n#\n# For high-volume tokenization with optimal batching, use databricks_skyflow.ipynb instead.\n#\n\ntokenize_function_sql = f\"\"\"\nCREATE OR REPLACE FUNCTION skyflow_tokenize(\n  value STRING,\n  table_name STRING,\n  column_name STRING\n)\nRETURNS STRING\nLANGUAGE PYTHON\nCOMMENT 'Tokenize a value using Skyflow Lambda API (scalar function)'\nAS $$\nimport requests\nimport json\n\ndef tokenize(value, table_name, column_name):\n    if value is None:\n        return None\n\n    # Skyflow and Lambda configuration (embedded at function creation time)\n    CLUSTER_ID = '{CLUSTER_ID}'\n    VAULT_ID = '{VAULT_ID}'\n    LAMBDA_URL = '{LAMBDA_URL}'\n    REQUEST_TIMEOUT = {REQUEST_TIMEOUT}\n\n    # Prepare request (single value)\n    endpoint = LAMBDA_URL\n    payload = {{\n        \"records\": [{{column_name: value}}]\n    }}\n\n    headers = {{\n        \"Content-Type\": \"application/json\",\n        \"X-Skyflow-Operation\": \"tokenize\",\n        \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n        \"X-Skyflow-Vault-ID\": VAULT_ID,\n        \"X-Skyflow-Table\": table_name\n    }}\n\n    # Call Lambda API\n    response = requests.post(endpoint, json=payload, headers=headers, timeout=REQUEST_TIMEOUT)\n    response.raise_for_status()\n\n    # Extract token from response\n    data = response.json().get(\"data\", [])\n    if data and len(data) > 0:\n        return data[0].get(column_name)\n\n    return None\n\nreturn tokenize(value, table_name, column_name)\n$$\n\"\"\"\n\nspark.sql(tokenize_function_sql)\n\nprint(\"✓ Created function: skyflow_tokenize(value, table_name, column_name)\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint(f\"  Cluster ID: {CLUSTER_ID}\")\nprint(f\"  Vault ID: {VAULT_ID}\")\nprint(f\"  Timeout: {REQUEST_TIMEOUT}s\")\nprint(\"\")\nprint(\"⚠️  NOTE: This is a scalar function (1 API call per row)\")\nprint(\"   Spark parallelizes across partitions for performance\")\nprint(\"   For optimal batching, use databricks_skyflow.ipynb with Pandas UDFs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Step 3: Create Detokenization External Function\n# ============================================================================\n#\n# NOTE: This is also a SCALAR function (1 API call per token).\n#\n\ndetokenize_function_sql = f\"\"\"\nCREATE OR REPLACE FUNCTION skyflow_detokenize(token STRING)\nRETURNS STRING\nLANGUAGE PYTHON\nCOMMENT 'Detokenize a token using Skyflow Lambda API (scalar function)'\nAS $$\nimport requests\nimport json\n\ndef detokenize(token):\n    if token is None:\n        return None\n\n    # Skyflow and Lambda configuration (embedded at function creation time)\n    CLUSTER_ID = '{CLUSTER_ID}'\n    VAULT_ID = '{VAULT_ID}'\n    LAMBDA_URL = '{LAMBDA_URL}'\n    REQUEST_TIMEOUT = {REQUEST_TIMEOUT}\n\n    # Prepare request (single token)\n    endpoint = LAMBDA_URL\n    payload = {{\n        \"tokens\": [token]\n    }}\n\n    headers = {{\n        \"Content-Type\": \"application/json\",\n        \"X-Skyflow-Operation\": \"detokenize\",\n        \"X-Skyflow-Cluster-ID\": CLUSTER_ID,\n        \"X-Skyflow-Vault-ID\": VAULT_ID\n    }}\n\n    # Call Lambda API\n    response = requests.post(endpoint, json=payload, headers=headers, timeout=REQUEST_TIMEOUT)\n    response.raise_for_status()\n\n    # Extract value from response\n    data = response.json().get(\"data\", [])\n    if data and len(data) > 0:\n        return data[0].get(\"value\")\n\n    return None\n\nreturn detokenize(token)\n$$\n\"\"\"\n\nspark.sql(detokenize_function_sql)\n\nprint(\"✓ Created function: skyflow_detokenize(token)\")\nprint(f\"  Lambda URL: {LAMBDA_URL}\")\nprint(f\"  Cluster ID: {CLUSTER_ID}\")\nprint(f\"  Vault ID: {VAULT_ID}\")\nprint(f\"  Timeout: {REQUEST_TIMEOUT}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Complete!\n",
    "\n",
    "The following persistent functions have been created in Unity Catalog:\n",
    "\n",
    "1. **skyflow_tokenize(value, table_name, column_name)** - Tokenizes sensitive data\n",
    "2. **skyflow_detokenize(token)** - Detokenizes tokens back to plaintext\n",
    "\n",
    "These functions are now:\n",
    "- Available to all users with EXECUTE permission\n",
    "- Persistent across cluster restarts\n",
    "- Configured with your Cluster ID, Vault ID, and Lambda URL\n",
    "\n",
    "**Optional:** Grant permissions to other users:\n",
    "```sql\n",
    "GRANT EXECUTE ON FUNCTION skyflow_tokenize TO `data_engineers`;\n",
    "GRANT EXECUTE ON FUNCTION skyflow_detokenize TO `data_engineers`;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Usage Examples\n",
    "\n",
    "The cells below demonstrate how to use the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Functions Exist\n",
    "\n",
    "Check that the external functions were created successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all functions in the schema\n",
    "functions_df = spark.sql(f\"\"\"\n",
    "    SHOW FUNCTIONS IN {CATALOG}.{SCHEMA}\n",
    "    LIKE 'skyflow*'\n",
    "\"\"\")\n",
    "\n",
    "display(functions_df)\n",
    "\n",
    "# Verify both functions exist\n",
    "function_names = [row.function for row in functions_df.collect()]\n",
    "expected = [\n",
    "    f\"{CATALOG}.{SCHEMA}.skyflow_tokenize\",\n",
    "    f\"{CATALOG}.{SCHEMA}.skyflow_detokenize\"\n",
    "]\n",
    "\n",
    "for func in expected:\n",
    "    if func in function_names:\n",
    "        print(f\"✓ Found: {func}\")\n",
    "    else:\n",
    "        print(f\"✗ Missing: {func}\")\n",
    "        print(f\"  Re-run cells 2-3 to create functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "\n",
    "Create sample data for testing the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, current_timestamp\n",
    "\n",
    "# Configure number of test rows\n",
    "NUM_ROWS = 100\n",
    "\n",
    "# Create test data\n",
    "test_df = spark.range(NUM_ROWS).select(\n",
    "    (expr(\"id + 1\").alias(\"user_id\")),\n",
    "    expr(\"concat('user_', id)\").alias(\"username\"),\n",
    "    expr(\"concat('user', id, '@example.com')\").alias(\"email\"),\n",
    "    expr(\"concat('555-01-', LPAD(id % 10000, 4, '0'))\").alias(\"ssn\"),\n",
    "    expr(\"concat('+1-555-', LPAD(id % 1000, 3, '0'), '-', LPAD((id * 7) % 10000, 4, '0'))\").alias(\"phone\"),\n",
    "    current_timestamp().alias(\"created_at\")\n",
    ")\n",
    "\n",
    "# Save as table\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(\"raw_users\")\n",
    "\n",
    "print(f\"✓ Created raw_users table with {NUM_ROWS} rows\")\n",
    "display(spark.table(\"raw_users\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Single Column\n",
    "\n",
    "Test the tokenization function on sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize emails using the persistent external function\n",
    "tokenized_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        user_id,\n",
    "        username,\n",
    "        email,\n",
    "        skyflow_tokenize(email, '{TABLE}', 'email') as email_token,\n",
    "        phone\n",
    "    FROM raw_users\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Persistent Tokenized Table\n",
    "\n",
    "Unlike temporary UDFs, Unity Catalog functions can create **permanent tables**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create persistent tokenized table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE tokenized_users AS\n",
    "    SELECT\n",
    "        user_id,\n",
    "        username,\n",
    "        skyflow_tokenize(email, '{TABLE}', 'email') as email_token,\n",
    "        phone,\n",
    "        created_at\n",
    "    FROM raw_users\n",
    "\"\"\")\n",
    "\n",
    "count = spark.table(\"tokenized_users\").count()\n",
    "print(f\"✓ Created tokenized_users table with {count} rows\")\n",
    "display(spark.table(\"tokenized_users\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detokenize Tokens\n",
    "\n",
    "Test the detokenization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize tokens\n",
    "detokenized_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        user_id,\n",
    "        username,\n",
    "        email_token,\n",
    "        skyflow_detokenize(email_token) as email_detokenized,\n",
    "        phone\n",
    "    FROM tokenized_users\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(detokenized_df)\n",
    "print(\"\\n✓ Roundtrip complete: Tokens → Detokenized values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Persistent Detokenized View\n",
    "\n",
    "**This is the key advantage**: Unity Catalog functions allow **persistent views**, unlike temporary UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PERSISTENT view (not possible with temporary UDFs!)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW users_detokenized AS\n",
    "    SELECT\n",
    "        user_id,\n",
    "        username,\n",
    "        skyflow_detokenize(email_token) as email,\n",
    "        phone,\n",
    "        created_at\n",
    "    FROM tokenized_users\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Created PERSISTENT view: users_detokenized\")\n",
    "print(\"  This view is available to all users and survives cluster restarts!\")\n",
    "display(spark.sql(\"SELECT * FROM users_detokenized LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Roundtrip Accuracy\n",
    "\n",
    "Compare original values with tokenized and detokenized values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs detokenized\n",
    "verification_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        t.user_id,\n",
    "        r.email as original_email,\n",
    "        t.email_token,\n",
    "        d.email as detokenized_email,\n",
    "        CASE\n",
    "            WHEN r.email = d.email THEN 'MATCH'\n",
    "            ELSE 'MISMATCH'\n",
    "        END as verification\n",
    "    FROM tokenized_users t\n",
    "    JOIN raw_users r ON t.user_id = r.user_id\n",
    "    JOIN users_detokenized d ON t.user_id = d.user_id\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(verification_df)\n",
    "\n",
    "# Check for any mismatches\n",
    "mismatches = verification_df.filter(\"verification = 'MISMATCH'\").count()\n",
    "if mismatches == 0:\n",
    "    print(\"\\n✓ All records match! Tokenization → Detokenization working correctly.\")\n",
    "else:\n",
    "    print(f\"\\n✗ Found {mismatches} mismatches - investigate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "To remove all test resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to drop test tables and views\n",
    "# spark.sql(\"DROP TABLE IF EXISTS raw_users\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS tokenized_users\")\n",
    "# spark.sql(\"DROP VIEW IF EXISTS users_detokenized\")\n",
    "# spark.sql(\"DROP FUNCTION IF EXISTS skyflow_detokenize\")\n",
    "# spark.sql(\"DROP FUNCTION IF EXISTS skyflow_tokenize\")\n",
    "# print(\"✓ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated Unity Catalog external functions for Skyflow integration:\n",
    "\n",
    "**Key Features:**\n",
    "- Simple setup - Create functions directly from this notebook (cells 1-3)  \n",
    "- No connection object needed - Lambda URL embedded directly in functions\n",
    "- Persistent functions - Available across all sessions and users  \n",
    "- Persistent views - Create permanent detokenized views  \n",
    "- Production-ready - Suitable for ETL pipelines and production workloads  \n",
    "- Access control - Leverage Unity Catalog permissions + Skyflow governance  \n",
    "- Embedded configuration - Cluster ID, Vault ID, and Lambda URL hardcoded in functions\n",
    "\n",
    "**Important Performance Note:**\n",
    "\n",
    "Unity Catalog SQL UDFs are **scalar functions** that process one row at a time:\n",
    "- Makes 1 API call per row (not batched)\n",
    "- Spark parallelizes across partitions for performance\n",
    "- Suitable for moderate-volume workloads where persistence is more important than API call optimization\n",
    "\n",
    "**When to use Unity Catalog vs Temporary UDFs:**\n",
    "\n",
    "| Use Case | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Production pipelines, shared resources | Unity Catalog (this notebook) |\n",
    "| High-volume tokenization (1M+ rows) | Temporary Pandas UDFs (databricks_skyflow.ipynb) |\n",
    "| Ad-hoc analysis, development | Temporary Pandas UDFs (databricks_skyflow.ipynb) |\n",
    "| Persistent views required | Unity Catalog (this notebook) |\n",
    "| Optimal API call batching required | Temporary Pandas UDFs (databricks_skyflow.ipynb) |\n",
    "\n",
    "**To update configuration:**\n",
    "1. Update variables in cell 1\n",
    "2. Re-run cells 2-3 to recreate functions with new configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}